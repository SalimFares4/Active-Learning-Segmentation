{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "from models import *\n",
    "from strategies import *\n",
    "from custom_datasets import *\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "import tqdm\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "import segmentation_models_pytorch as smp\n",
    "import os\n",
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "import torch.nn as nn\n",
    "\n",
    "from PIL import Image\n",
    "import torchvision.transforms as T\n",
    "import torchvision.transforms.functional as TF\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "\n",
    "import time\n",
    "import json\n",
    "\n",
    "import threading\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_path = \"/root/Master_Thesis/\"\n",
    "dataframes_path = main_path + \"data/dataframes/\"\n",
    "sam_path = main_path + \"sam/sam_vit_h_4b8939.pth\"\n",
    "expirements_path = main_path+\"expirements/\"\n",
    "models_path = main_path + \"scripts/notebooks/trained_models/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_name = \"brain_df\"\n",
    "train_df = pd.read_csv(dataframes_path+\"brain_df_train.csv\")\n",
    "test_df = pd.read_csv(dataframes_path+\"brain_df_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'n_epoch': 35,\n",
    "          'train_args':{'batch_size': 4, 'num_workers': 1},\n",
    "          'test_args':{'batch_size': 512, 'num_workers': 1},\n",
    "          'optimizer_args':{'lr': 5e-3, 'momentum': 0.9},\n",
    "          'use_sam': True,\n",
    "          'use_predictor': True,\n",
    "          'use_generator': False,\n",
    "          'init_set_size': 100,\n",
    "          'query_num': 5, #int(0.1*len(test_df)),\n",
    "          'rounds': 21,\n",
    "          \"activate_sam_at_round\":1, \n",
    "          \"img_size\":(128, 128)}\n",
    "          \n",
    "params['test_set_size'] = len(test_df)\n",
    "params['df'] = df_name\n",
    "params[\"strategy\"] = \"MarginSampling\"\n",
    "params[\"voters\"] = f'trained_models/voters/voters_{params[\"img_size\"][0]}_'\n",
    "init_main_state_path = f'trained_models/voters/voters_{params[\"img_size\"][0]}_0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(handler, train_df, test_df):\n",
    "    return Data(train_df[\"images\"].to_list(), train_df[\"masks\"].to_list(), test_df[\"images\"].to_list(), test_df[\"masks\"].to_list(), handler, img_size=params[\"img_size\"], df=train_df, path= main_path+\"/data/processed/\", use_sam=params['use_sam'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_data(Handler, train_df, test_df)\n",
    "data.initialize_labels(params[\"init_set_size\"])\n",
    "results=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in [1,2,3,4,5,6,7,8,9,10]:\n",
    "#     model = smp.create_model(\n",
    "#             'Unet', encoder_name='resnet34', in_channels=3, classes = 1\n",
    "#         )\n",
    "#     torch.save(model.state_dict(), f\"trained_models/voters/voters_128_0/model_{i}.pt\")\n",
    "#     print(f\"Model_{i}'s training saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def ensemble(models_num, starting_index, params, data, cuurent_round=1, query_idxs=None):\n",
    "    for i in range(starting_index, models_num+starting_index):\n",
    "        print(f\"Model_{i}'s training started!\", flush=True)\n",
    "        model = smp.create_model(\n",
    "                'Unet', encoder_name='resnet34', in_channels=3, classes = 1\n",
    "            )\n",
    "        init_state_Unet = torch.load(f\"trained_models/voters/voters_128_0/model_{i}.pt\")\n",
    "        net = Net(model, params, device = torch.device(\"cuda\"))\n",
    "        net.net.load_state_dict(init_state_Unet)\n",
    "        strategy = MarginSampling(dataset=data, net=net, sam=None, params=params)\n",
    "        if not query_idxs is None:\n",
    "            strategy.update(query_idxs)\n",
    "        \n",
    "        strategy.train()\n",
    "        torch.save( strategy.net.net.state_dict(), f'{params[\"voters\"]}{cuurent_round}/model_{i}.pt')\n",
    "        logits, mask_gt = strategy.predict(data.get_test_data())\n",
    "        iou_score, accuracy, precision, recall, f1_score = data.cal_test_metrics(logits, mask_gt )\n",
    "        print(f\"Testing metrics for model_{i}: iou_score = {iou_score:.2f}, accuracy = {accuracy:.2f}, precision = {precision:.2f}, recall = {recall:.2f}, f1_score = {f1_score:.2f}\", flush=True)\n",
    "        print(f\"Model_{i}'s saved!\", flush=True)\n",
    "    # print(\"Done!\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_idxs = [1656,  121,  253,  968, 2095]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensemble(models_num=1, starting_index=3, params=params, data=data, cuurent_round=1, query_idxs=query_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # for i in range(1, 11):\n",
    "# for i in [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]:\n",
    "#     t = threading.Thread(target=ensemble, daemon=True, args=[1, i, params, data, 1, query_idxs])\n",
    "#     t.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t1 = threading.Thread(target=ensemble, daemon=True, args=[5, 1, params, data, query_idxs])\n",
    "# t1.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t2 = threading.Thread(target=ensemble, daemon=True, args=[5, 6, params, data, query_idxs])\n",
    "# t2.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = smp.create_model(\n",
    "        'Unet', encoder_name='resnet34', in_channels=3, classes = 1\n",
    "    )\n",
    "net = Net(model, params, device = torch.device(\"cuda\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "No_Active_128_Unet = main_path + \"scripts/notebooks/trained_models/No_Active_128_Unet.pt\"\n",
    "not_voters_Unet_128_done = main_path + \"scripts/notebooks/trained_models/not_pre_trained_Unet_128_done.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(net, idx, params, data, current_round=1):\n",
    "    for i in idx:\n",
    "        net.net.load_state_dict(torch.load(f'{params[\"voters\"]}{current_round}/model_{i}.pt'))\n",
    "        logits, mask_gt = net.predict(data.get_test_data())\n",
    "        iou_score, accuracy, precision, recall, f1_score = data.cal_test_metrics(logits, mask_gt )\n",
    "        print((f\"Testing metrics for model_{i}: iou_score = {iou_score:.2f}, accuracy = {accuracy:.2f}, precision = {precision:.2f}, recall = {recall:.2f}, f1_score = {f1_score:.2f}\"))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = [i for i in range(1,11)]\n",
    "idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing metrics for model_1: iou_score = 0.01, accuracy = 0.72, precision = 0.01, recall = 0.19, f1_score = 0.01\n",
      "Testing metrics for model_2: iou_score = 0.01, accuracy = 0.08, precision = 0.01, recall = 0.96, f1_score = 0.02\n",
      "Testing metrics for model_3: iou_score = 0.00, accuracy = 0.97, precision = 0.00, recall = 0.00, f1_score = 0.00\n",
      "Testing metrics for model_4: iou_score = 0.01, accuracy = 0.43, precision = 0.01, recall = 0.64, f1_score = 0.02\n",
      "Testing metrics for model_5: iou_score = 0.01, accuracy = 0.11, precision = 0.01, recall = 0.87, f1_score = 0.02\n",
      "Testing metrics for model_6: iou_score = 0.01, accuracy = 0.01, precision = 0.01, recall = 1.00, f1_score = 0.02\n",
      "Testing metrics for model_7: iou_score = 0.01, accuracy = 0.90, precision = 0.01, recall = 0.05, f1_score = 0.01\n",
      "Testing metrics for model_8: iou_score = 0.01, accuracy = 0.85, precision = 0.01, recall = 0.09, f1_score = 0.01\n",
      "Testing metrics for model_9: iou_score = 0.01, accuracy = 0.01, precision = 0.01, recall = 1.00, f1_score = 0.02\n",
      "Testing metrics for model_10: iou_score = 0.00, accuracy = 0.98, precision = 0.00, recall = 0.00, f1_score = 0.00\n"
     ]
    }
   ],
   "source": [
    "test(net, idx, params, data, current_round=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing metrics for model_1: iou_score = 0.47, accuracy = 0.99, precision = 0.64, recall = 0.65, f1_score = 0.64\n",
      "Testing metrics for model_2: iou_score = 0.50, accuracy = 0.99, precision = 0.61, recall = 0.73, f1_score = 0.67\n",
      "Testing metrics for model_3: iou_score = 0.53, accuracy = 0.99, precision = 0.69, recall = 0.70, f1_score = 0.70\n",
      "Testing metrics for model_4: iou_score = 0.52, accuracy = 0.99, precision = 0.69, recall = 0.67, f1_score = 0.68\n",
      "Testing metrics for model_5: iou_score = 0.44, accuracy = 0.99, precision = 0.72, recall = 0.52, f1_score = 0.61\n",
      "Testing metrics for model_6: iou_score = 0.47, accuracy = 0.99, precision = 0.64, recall = 0.64, f1_score = 0.64\n",
      "Testing metrics for model_7: iou_score = 0.55, accuracy = 0.99, precision = 0.68, recall = 0.73, f1_score = 0.71\n",
      "Testing metrics for model_8: iou_score = 0.45, accuracy = 0.99, precision = 0.75, recall = 0.54, f1_score = 0.62\n",
      "Testing metrics for model_9: iou_score = 0.47, accuracy = 0.99, precision = 0.53, recall = 0.81, f1_score = 0.64\n",
      "Testing metrics for model_10: iou_score = 0.48, accuracy = 0.99, precision = 0.79, recall = 0.55, f1_score = 0.65\n"
     ]
    }
   ],
   "source": [
    "test(net, idx, params, data, current_round=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing metrics for model_1: iou_score = 0.47, accuracy = 0.99, precision = 0.77, recall = 0.55, f1_score = 0.64\n",
      "Testing metrics for model_2: iou_score = 0.45, accuracy = 0.99, precision = 0.81, recall = 0.51, f1_score = 0.63\n",
      "Testing metrics for model_3: iou_score = 0.50, accuracy = 0.99, precision = 0.80, recall = 0.57, f1_score = 0.66\n",
      "Testing metrics for model_4: iou_score = 0.51, accuracy = 0.99, precision = 0.72, recall = 0.63, f1_score = 0.68\n",
      "Testing metrics for model_5: iou_score = 0.53, accuracy = 0.99, precision = 0.64, recall = 0.75, f1_score = 0.69\n",
      "Testing metrics for model_6: iou_score = 0.54, accuracy = 0.99, precision = 0.64, recall = 0.77, f1_score = 0.70\n",
      "Testing metrics for model_7: iou_score = 0.44, accuracy = 0.99, precision = 0.79, recall = 0.50, f1_score = 0.62\n",
      "Testing metrics for model_8: iou_score = 0.45, accuracy = 0.99, precision = 0.71, recall = 0.56, f1_score = 0.62\n",
      "Testing metrics for model_9: iou_score = 0.49, accuracy = 0.99, precision = 0.81, recall = 0.56, f1_score = 0.66\n",
      "Testing metrics for model_10: iou_score = 0.47, accuracy = 0.99, precision = 0.84, recall = 0.51, f1_score = 0.64\n"
     ]
    }
   ],
   "source": [
    "test(net, idx, params, data, current_round=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing metrics for model_1: iou_score = 0.49, accuracy = 0.99, precision = 0.58, recall = 0.77, f1_score = 0.66\n",
      "Testing metrics for model_2: iou_score = 0.55, accuracy = 0.99, precision = 0.74, recall = 0.68, f1_score = 0.71\n",
      "Testing metrics for model_3: iou_score = 0.54, accuracy = 0.99, precision = 0.82, recall = 0.61, f1_score = 0.70\n",
      "Testing metrics for model_4: iou_score = 0.52, accuracy = 0.99, precision = 0.71, recall = 0.66, f1_score = 0.69\n",
      "Testing metrics for model_5: iou_score = 0.46, accuracy = 0.99, precision = 0.86, recall = 0.49, f1_score = 0.63\n",
      "Testing metrics for model_6: iou_score = 0.50, accuracy = 0.99, precision = 0.69, recall = 0.64, f1_score = 0.66\n",
      "Testing metrics for model_7: iou_score = 0.50, accuracy = 0.99, precision = 0.68, recall = 0.64, f1_score = 0.66\n",
      "Testing metrics for model_8: iou_score = 0.48, accuracy = 0.99, precision = 0.75, recall = 0.57, f1_score = 0.65\n",
      "Testing metrics for model_9: iou_score = 0.54, accuracy = 0.99, precision = 0.71, recall = 0.70, f1_score = 0.71\n",
      "Testing metrics for model_10: iou_score = 0.53, accuracy = 0.99, precision = 0.62, recall = 0.79, f1_score = 0.70\n"
     ]
    }
   ],
   "source": [
    "test(net, idx, params, data, current_round=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing metrics for model_1: iou_score = 0.50, accuracy = 0.99, precision = 0.59, recall = 0.76, f1_score = 0.67\n",
      "Testing metrics for model_2: iou_score = 0.44, accuracy = 0.99, precision = 0.85, recall = 0.48, f1_score = 0.61\n",
      "Testing metrics for model_3: iou_score = 0.51, accuracy = 0.99, precision = 0.78, recall = 0.59, f1_score = 0.67\n",
      "Testing metrics for model_4: iou_score = 0.54, accuracy = 0.99, precision = 0.80, recall = 0.63, f1_score = 0.70\n",
      "Testing metrics for model_5: iou_score = 0.53, accuracy = 0.99, precision = 0.72, recall = 0.68, f1_score = 0.70\n",
      "Testing metrics for model_6: iou_score = 0.56, accuracy = 0.99, precision = 0.71, recall = 0.72, f1_score = 0.72\n",
      "Testing metrics for model_7: iou_score = 0.47, accuracy = 0.99, precision = 0.88, recall = 0.51, f1_score = 0.64\n",
      "Testing metrics for model_8: iou_score = 0.55, accuracy = 0.99, precision = 0.69, recall = 0.73, f1_score = 0.71\n",
      "Testing metrics for model_9: iou_score = 0.53, accuracy = 0.99, precision = 0.79, recall = 0.62, f1_score = 0.70\n",
      "Testing metrics for model_10: iou_score = 0.51, accuracy = 0.99, precision = 0.78, recall = 0.60, f1_score = 0.68\n"
     ]
    }
   ],
   "source": [
    "test(net, idx, params, data, current_round=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing metrics for model_1: iou_score = 0.45, accuracy = 0.99, precision = 0.83, recall = 0.50, f1_score = 0.63\n",
      "Testing metrics for model_2: iou_score = 0.54, accuracy = 0.99, precision = 0.78, recall = 0.63, f1_score = 0.70\n",
      "Testing metrics for model_3: iou_score = 0.54, accuracy = 0.99, precision = 0.74, recall = 0.66, f1_score = 0.70\n",
      "Testing metrics for model_4: iou_score = 0.51, accuracy = 0.99, precision = 0.63, recall = 0.73, f1_score = 0.68\n",
      "Testing metrics for model_5: iou_score = 0.55, accuracy = 0.99, precision = 0.68, recall = 0.74, f1_score = 0.71\n",
      "Testing metrics for model_6: iou_score = 0.54, accuracy = 0.99, precision = 0.68, recall = 0.71, f1_score = 0.70\n",
      "Testing metrics for model_7: iou_score = 0.51, accuracy = 0.99, precision = 0.82, recall = 0.58, f1_score = 0.68\n",
      "Testing metrics for model_8: iou_score = 0.57, accuracy = 0.99, precision = 0.71, recall = 0.74, f1_score = 0.72\n",
      "Testing metrics for model_9: iou_score = 0.56, accuracy = 0.99, precision = 0.74, recall = 0.70, f1_score = 0.72\n",
      "Testing metrics for model_10: iou_score = 0.57, accuracy = 0.99, precision = 0.76, recall = 0.69, f1_score = 0.72\n"
     ]
    }
   ],
   "source": [
    "test(net, idx, params, data, current_round=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing metrics for model_1: iou_score = 0.51, accuracy = 0.99, precision = 0.80, recall = 0.58, f1_score = 0.67\n",
      "Testing metrics for model_2: iou_score = 0.54, accuracy = 0.99, precision = 0.82, recall = 0.61, f1_score = 0.70\n",
      "Testing metrics for model_3: iou_score = 0.55, accuracy = 0.99, precision = 0.75, recall = 0.68, f1_score = 0.71\n",
      "Testing metrics for model_4: iou_score = 0.57, accuracy = 0.99, precision = 0.76, recall = 0.69, f1_score = 0.72\n",
      "Testing metrics for model_5: iou_score = 0.54, accuracy = 0.99, precision = 0.74, recall = 0.66, f1_score = 0.70\n",
      "Testing metrics for model_6: iou_score = 0.52, accuracy = 0.99, precision = 0.82, recall = 0.58, f1_score = 0.68\n",
      "Testing metrics for model_7: iou_score = 0.55, accuracy = 0.99, precision = 0.76, recall = 0.66, f1_score = 0.71\n",
      "Testing metrics for model_8: iou_score = 0.52, accuracy = 0.99, precision = 0.84, recall = 0.57, f1_score = 0.68\n",
      "Testing metrics for model_9: iou_score = 0.50, accuracy = 0.99, precision = 0.81, recall = 0.57, f1_score = 0.66\n",
      "Testing metrics for model_10: iou_score = 0.48, accuracy = 0.99, precision = 0.58, recall = 0.75, f1_score = 0.65\n"
     ]
    }
   ],
   "source": [
    "test(net, idx, params, data, current_round=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing metrics for model_1: iou_score = 0.55, accuracy = 0.99, precision = 0.76, recall = 0.67, f1_score = 0.71\n",
      "Testing metrics for model_2: iou_score = 0.54, accuracy = 0.99, precision = 0.83, recall = 0.61, f1_score = 0.70\n",
      "Testing metrics for model_3: iou_score = 0.48, accuracy = 0.99, precision = 0.78, recall = 0.55, f1_score = 0.65\n",
      "Testing metrics for model_4: iou_score = 0.50, accuracy = 0.99, precision = 0.82, recall = 0.56, f1_score = 0.67\n",
      "Testing metrics for model_5: iou_score = 0.54, accuracy = 0.99, precision = 0.70, recall = 0.69, f1_score = 0.70\n",
      "Testing metrics for model_6: iou_score = 0.45, accuracy = 0.99, precision = 0.54, recall = 0.72, f1_score = 0.62\n",
      "Testing metrics for model_7: iou_score = 0.48, accuracy = 0.99, precision = 0.86, recall = 0.52, f1_score = 0.64\n",
      "Testing metrics for model_8: iou_score = 0.51, accuracy = 0.99, precision = 0.81, recall = 0.57, f1_score = 0.67\n",
      "Testing metrics for model_9: iou_score = 0.53, accuracy = 0.99, precision = 0.86, recall = 0.58, f1_score = 0.69\n",
      "Testing metrics for model_10: iou_score = 0.50, accuracy = 0.99, precision = 0.84, recall = 0.55, f1_score = 0.67\n"
     ]
    }
   ],
   "source": [
    "test(net, idx, params, data, current_round=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing metrics for model_1: iou_score = 0.55, accuracy = 0.99, precision = 0.81, recall = 0.63, f1_score = 0.71\n",
      "Testing metrics for model_2: iou_score = 0.51, accuracy = 0.99, precision = 0.86, recall = 0.56, f1_score = 0.68\n",
      "Testing metrics for model_3: iou_score = 0.51, accuracy = 0.99, precision = 0.78, recall = 0.59, f1_score = 0.68\n",
      "Testing metrics for model_4: iou_score = 0.50, accuracy = 0.99, precision = 0.71, recall = 0.63, f1_score = 0.67\n",
      "Testing metrics for model_5: iou_score = 0.52, accuracy = 0.99, precision = 0.82, recall = 0.59, f1_score = 0.69\n",
      "Testing metrics for model_6: iou_score = 0.49, accuracy = 0.99, precision = 0.73, recall = 0.59, f1_score = 0.66\n",
      "Testing metrics for model_7: iou_score = 0.53, accuracy = 0.99, precision = 0.81, recall = 0.61, f1_score = 0.70\n",
      "Testing metrics for model_8: iou_score = 0.56, accuracy = 0.99, precision = 0.83, recall = 0.63, f1_score = 0.72\n",
      "Testing metrics for model_9: iou_score = 0.50, accuracy = 0.99, precision = 0.84, recall = 0.56, f1_score = 0.67\n",
      "Testing metrics for model_10: iou_score = 0.45, accuracy = 0.99, precision = 0.91, recall = 0.47, f1_score = 0.62\n"
     ]
    }
   ],
   "source": [
    "test(net, idx, params, data, current_round=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing metrics for model_1: iou_score = 0.58, accuracy = 0.99, precision = 0.76, recall = 0.71, f1_score = 0.73\n",
      "Testing metrics for model_2: iou_score = 0.56, accuracy = 0.99, precision = 0.80, recall = 0.65, f1_score = 0.72\n",
      "Testing metrics for model_3: iou_score = 0.56, accuracy = 0.99, precision = 0.77, recall = 0.67, f1_score = 0.72\n",
      "Testing metrics for model_4: iou_score = 0.57, accuracy = 0.99, precision = 0.74, recall = 0.70, f1_score = 0.72\n",
      "Testing metrics for model_5: iou_score = 0.50, accuracy = 0.99, precision = 0.87, recall = 0.54, f1_score = 0.67\n",
      "Testing metrics for model_6: iou_score = 0.50, accuracy = 0.99, precision = 0.90, recall = 0.53, f1_score = 0.67\n",
      "Testing metrics for model_7: iou_score = 0.55, accuracy = 0.99, precision = 0.84, recall = 0.61, f1_score = 0.71\n",
      "Testing metrics for model_8: iou_score = 0.53, accuracy = 0.99, precision = 0.79, recall = 0.62, f1_score = 0.69\n",
      "Testing metrics for model_9: iou_score = 0.52, accuracy = 0.99, precision = 0.85, recall = 0.58, f1_score = 0.69\n",
      "Testing metrics for model_10: iou_score = 0.55, accuracy = 0.99, precision = 0.73, recall = 0.68, f1_score = 0.71\n"
     ]
    }
   ],
   "source": [
    "test(net, idx, params, data, current_round=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing metrics for model_1: iou_score = 0.51, accuracy = 0.99, precision = 0.83, recall = 0.57, f1_score = 0.67\n",
      "Testing metrics for model_2: iou_score = 0.55, accuracy = 0.99, precision = 0.83, recall = 0.62, f1_score = 0.71\n",
      "Testing metrics for model_3: iou_score = 0.49, accuracy = 0.99, precision = 0.86, recall = 0.54, f1_score = 0.66\n",
      "Testing metrics for model_4: iou_score = 0.58, accuracy = 0.99, precision = 0.83, recall = 0.66, f1_score = 0.74\n",
      "Testing metrics for model_5: iou_score = 0.52, accuracy = 0.99, precision = 0.88, recall = 0.56, f1_score = 0.69\n",
      "Testing metrics for model_6: iou_score = 0.57, accuracy = 0.99, precision = 0.76, recall = 0.69, f1_score = 0.72\n",
      "Testing metrics for model_7: iou_score = 0.53, accuracy = 0.99, precision = 0.87, recall = 0.57, f1_score = 0.69\n",
      "Testing metrics for model_8: iou_score = 0.53, accuracy = 0.99, precision = 0.82, recall = 0.60, f1_score = 0.70\n",
      "Testing metrics for model_9: iou_score = 0.54, accuracy = 0.99, precision = 0.76, recall = 0.65, f1_score = 0.70\n",
      "Testing metrics for model_10: iou_score = 0.50, accuracy = 0.99, precision = 0.64, recall = 0.71, f1_score = 0.67\n"
     ]
    }
   ],
   "source": [
    "test(net, idx, params, data, current_round=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing metrics for model_1: iou_score = 0.50, accuracy = 0.99, precision = 0.87, recall = 0.55, f1_score = 0.67\n",
      "Testing metrics for model_2: iou_score = 0.51, accuracy = 0.99, precision = 0.85, recall = 0.56, f1_score = 0.67\n",
      "Testing metrics for model_3: iou_score = 0.57, accuracy = 0.99, precision = 0.78, recall = 0.68, f1_score = 0.72\n",
      "Testing metrics for model_4: iou_score = 0.58, accuracy = 0.99, precision = 0.80, recall = 0.68, f1_score = 0.73\n",
      "Testing metrics for model_5: iou_score = 0.56, accuracy = 0.99, precision = 0.68, recall = 0.76, f1_score = 0.72\n",
      "Testing metrics for model_6: iou_score = 0.49, accuracy = 0.99, precision = 0.70, recall = 0.62, f1_score = 0.66\n",
      "Testing metrics for model_7: iou_score = 0.58, accuracy = 0.99, precision = 0.86, recall = 0.64, f1_score = 0.73\n",
      "Testing metrics for model_8: iou_score = 0.57, accuracy = 0.99, precision = 0.82, recall = 0.65, f1_score = 0.72\n",
      "Testing metrics for model_9: iou_score = 0.55, accuracy = 0.99, precision = 0.82, recall = 0.62, f1_score = 0.71\n",
      "Testing metrics for model_10: iou_score = 0.58, accuracy = 0.99, precision = 0.72, recall = 0.75, f1_score = 0.73\n"
     ]
    }
   ],
   "source": [
    "test(net, idx, params, data, current_round=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing metrics for model_1: iou_score = 0.56, accuracy = 0.99, precision = 0.82, recall = 0.65, f1_score = 0.72\n",
      "Testing metrics for model_2: iou_score = 0.56, accuracy = 0.99, precision = 0.71, recall = 0.71, f1_score = 0.71\n",
      "Testing metrics for model_3: iou_score = 0.54, accuracy = 0.99, precision = 0.78, recall = 0.63, f1_score = 0.70\n",
      "Testing metrics for model_4: iou_score = 0.54, accuracy = 0.99, precision = 0.63, recall = 0.80, f1_score = 0.70\n",
      "Testing metrics for model_5: iou_score = 0.51, accuracy = 0.99, precision = 0.85, recall = 0.56, f1_score = 0.68\n",
      "Testing metrics for model_6: iou_score = 0.55, accuracy = 0.99, precision = 0.75, recall = 0.68, f1_score = 0.71\n",
      "Testing metrics for model_7: iou_score = 0.54, accuracy = 0.99, precision = 0.65, recall = 0.77, f1_score = 0.70\n",
      "Testing metrics for model_8: iou_score = 0.55, accuracy = 0.99, precision = 0.74, recall = 0.68, f1_score = 0.71\n",
      "Testing metrics for model_9: iou_score = 0.53, accuracy = 0.99, precision = 0.82, recall = 0.60, f1_score = 0.69\n",
      "Testing metrics for model_10: iou_score = 0.57, accuracy = 0.99, precision = 0.79, recall = 0.67, f1_score = 0.73\n"
     ]
    }
   ],
   "source": [
    "test(net, idx, params, data, current_round=13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing metrics for model_1: iou_score = 0.58, accuracy = 0.99, precision = 0.80, recall = 0.68, f1_score = 0.73\n",
      "Testing metrics for model_2: iou_score = 0.51, accuracy = 0.99, precision = 0.85, recall = 0.56, f1_score = 0.67\n",
      "Testing metrics for model_3: iou_score = 0.49, accuracy = 0.99, precision = 0.86, recall = 0.53, f1_score = 0.66\n",
      "Testing metrics for model_4: iou_score = 0.55, accuracy = 0.99, precision = 0.81, recall = 0.64, f1_score = 0.71\n",
      "Testing metrics for model_5: iou_score = 0.56, accuracy = 0.99, precision = 0.82, recall = 0.64, f1_score = 0.72\n",
      "Testing metrics for model_6: iou_score = 0.56, accuracy = 0.99, precision = 0.77, recall = 0.67, f1_score = 0.72\n",
      "Testing metrics for model_7: iou_score = 0.50, accuracy = 0.99, precision = 0.82, recall = 0.56, f1_score = 0.67\n",
      "Testing metrics for model_8: iou_score = 0.49, accuracy = 0.99, precision = 0.88, recall = 0.53, f1_score = 0.66\n",
      "Testing metrics for model_9: iou_score = 0.56, accuracy = 0.99, precision = 0.85, recall = 0.63, f1_score = 0.72\n",
      "Testing metrics for model_10: iou_score = 0.57, accuracy = 0.99, precision = 0.66, recall = 0.79, f1_score = 0.72\n"
     ]
    }
   ],
   "source": [
    "test(net, idx, params, data, current_round=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing metrics for model_1: iou_score = 0.58, accuracy = 0.99, precision = 0.82, recall = 0.67, f1_score = 0.74\n",
      "Testing metrics for model_2: iou_score = 0.60, accuracy = 1.00, precision = 0.84, recall = 0.68, f1_score = 0.75\n",
      "Testing metrics for model_3: iou_score = 0.53, accuracy = 0.99, precision = 0.85, recall = 0.58, f1_score = 0.69\n",
      "Testing metrics for model_4: iou_score = 0.49, accuracy = 0.99, precision = 0.85, recall = 0.54, f1_score = 0.66\n",
      "Testing metrics for model_5: iou_score = 0.59, accuracy = 0.99, precision = 0.81, recall = 0.69, f1_score = 0.74\n",
      "Testing metrics for model_6: iou_score = 0.56, accuracy = 0.99, precision = 0.68, recall = 0.75, f1_score = 0.71\n",
      "Testing metrics for model_7: iou_score = 0.49, accuracy = 0.99, precision = 0.82, recall = 0.54, f1_score = 0.65\n",
      "Testing metrics for model_8: iou_score = 0.56, accuracy = 0.99, precision = 0.72, recall = 0.72, f1_score = 0.72\n",
      "Testing metrics for model_9: iou_score = 0.50, accuracy = 0.99, precision = 0.82, recall = 0.56, f1_score = 0.66\n",
      "Testing metrics for model_10: iou_score = 0.60, accuracy = 0.99, precision = 0.79, recall = 0.71, f1_score = 0.75\n"
     ]
    }
   ],
   "source": [
    "test(net, idx, params, data, current_round=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing metrics for model_1: iou_score = 0.58, accuracy = 0.99, precision = 0.76, recall = 0.71, f1_score = 0.73\n",
      "Testing metrics for model_2: iou_score = 0.58, accuracy = 0.99, precision = 0.83, recall = 0.66, f1_score = 0.73\n",
      "Testing metrics for model_3: iou_score = 0.58, accuracy = 0.99, precision = 0.80, recall = 0.68, f1_score = 0.74\n",
      "Testing metrics for model_4: iou_score = 0.53, accuracy = 0.99, precision = 0.84, recall = 0.60, f1_score = 0.70\n",
      "Testing metrics for model_5: iou_score = 0.60, accuracy = 1.00, precision = 0.82, recall = 0.69, f1_score = 0.75\n",
      "Testing metrics for model_6: iou_score = 0.56, accuracy = 0.99, precision = 0.74, recall = 0.70, f1_score = 0.72\n",
      "Testing metrics for model_7: iou_score = 0.59, accuracy = 1.00, precision = 0.86, recall = 0.65, f1_score = 0.74\n",
      "Testing metrics for model_8: iou_score = 0.57, accuracy = 0.99, precision = 0.76, recall = 0.70, f1_score = 0.73\n",
      "Testing metrics for model_9: iou_score = 0.60, accuracy = 0.99, precision = 0.74, recall = 0.76, f1_score = 0.75\n",
      "Testing metrics for model_10: iou_score = 0.59, accuracy = 0.99, precision = 0.78, recall = 0.70, f1_score = 0.74\n"
     ]
    }
   ],
   "source": [
    "test(net, idx, params, data, current_round=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing metrics for model_1: iou_score = 0.59, accuracy = 0.99, precision = 0.79, recall = 0.70, f1_score = 0.74\n",
      "Testing metrics for model_2: iou_score = 0.58, accuracy = 0.99, precision = 0.83, recall = 0.65, f1_score = 0.73\n",
      "Testing metrics for model_3: iou_score = 0.54, accuracy = 0.99, precision = 0.81, recall = 0.61, f1_score = 0.70\n",
      "Testing metrics for model_4: iou_score = 0.58, accuracy = 0.99, precision = 0.86, recall = 0.64, f1_score = 0.73\n",
      "Testing metrics for model_5: iou_score = 0.57, accuracy = 0.99, precision = 0.86, recall = 0.63, f1_score = 0.73\n",
      "Testing metrics for model_6: iou_score = 0.54, accuracy = 0.99, precision = 0.85, recall = 0.60, f1_score = 0.70\n",
      "Testing metrics for model_7: iou_score = 0.58, accuracy = 0.99, precision = 0.82, recall = 0.66, f1_score = 0.73\n",
      "Testing metrics for model_8: iou_score = 0.56, accuracy = 0.99, precision = 0.86, recall = 0.62, f1_score = 0.72\n",
      "Testing metrics for model_9: iou_score = 0.54, accuracy = 0.99, precision = 0.85, recall = 0.60, f1_score = 0.70\n",
      "Testing metrics for model_10: iou_score = 0.56, accuracy = 0.99, precision = 0.82, recall = 0.64, f1_score = 0.72\n"
     ]
    }
   ],
   "source": [
    "test(net, idx, params, data, current_round=17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing metrics for model_1: iou_score = 0.56, accuracy = 0.99, precision = 0.79, recall = 0.66, f1_score = 0.72\n",
      "Testing metrics for model_2: iou_score = 0.56, accuracy = 0.99, precision = 0.72, recall = 0.72, f1_score = 0.72\n",
      "Testing metrics for model_3: iou_score = 0.61, accuracy = 1.00, precision = 0.80, recall = 0.72, f1_score = 0.76\n",
      "Testing metrics for model_4: iou_score = 0.61, accuracy = 1.00, precision = 0.83, recall = 0.70, f1_score = 0.76\n",
      "Testing metrics for model_5: iou_score = 0.59, accuracy = 0.99, precision = 0.78, recall = 0.71, f1_score = 0.74\n",
      "Testing metrics for model_6: iou_score = 0.58, accuracy = 0.99, precision = 0.76, recall = 0.71, f1_score = 0.74\n",
      "Testing metrics for model_7: iou_score = 0.56, accuracy = 0.99, precision = 0.82, recall = 0.64, f1_score = 0.72\n",
      "Testing metrics for model_8: iou_score = 0.56, accuracy = 0.99, precision = 0.81, recall = 0.65, f1_score = 0.72\n",
      "Testing metrics for model_9: iou_score = 0.59, accuracy = 0.99, precision = 0.80, recall = 0.69, f1_score = 0.74\n",
      "Testing metrics for model_10: iou_score = 0.56, accuracy = 0.99, precision = 0.84, recall = 0.63, f1_score = 0.72\n"
     ]
    }
   ],
   "source": [
    "test(net, idx, params, data, current_round=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing metrics for model_1: iou_score = 0.58, accuracy = 0.99, precision = 0.76, recall = 0.71, f1_score = 0.73\n",
      "Testing metrics for model_2: iou_score = 0.59, accuracy = 0.99, precision = 0.82, recall = 0.67, f1_score = 0.74\n",
      "Testing metrics for model_3: iou_score = 0.62, accuracy = 1.00, precision = 0.82, recall = 0.72, f1_score = 0.76\n",
      "Testing metrics for model_4: iou_score = 0.58, accuracy = 0.99, precision = 0.78, recall = 0.69, f1_score = 0.73\n",
      "Testing metrics for model_5: iou_score = 0.59, accuracy = 0.99, precision = 0.79, recall = 0.69, f1_score = 0.74\n",
      "Testing metrics for model_6: iou_score = 0.57, accuracy = 0.99, precision = 0.78, recall = 0.68, f1_score = 0.73\n",
      "Testing metrics for model_7: iou_score = 0.58, accuracy = 0.99, precision = 0.81, recall = 0.68, f1_score = 0.74\n",
      "Testing metrics for model_8: iou_score = 0.60, accuracy = 0.99, precision = 0.80, recall = 0.70, f1_score = 0.75\n",
      "Testing metrics for model_9: iou_score = 0.56, accuracy = 0.99, precision = 0.79, recall = 0.66, f1_score = 0.72\n",
      "Testing metrics for model_10: iou_score = 0.55, accuracy = 0.99, precision = 0.82, recall = 0.63, f1_score = 0.71\n"
     ]
    }
   ],
   "source": [
    "test(net, idx, params, data, current_round=19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing metrics for model_1: iou_score = 0.60, accuracy = 0.99, precision = 0.74, recall = 0.76, f1_score = 0.75\n",
      "Testing metrics for model_2: iou_score = 0.61, accuracy = 1.00, precision = 0.81, recall = 0.71, f1_score = 0.76\n",
      "Testing metrics for model_3: iou_score = 0.58, accuracy = 0.99, precision = 0.84, recall = 0.65, f1_score = 0.73\n",
      "Testing metrics for model_4: iou_score = 0.60, accuracy = 0.99, precision = 0.75, recall = 0.74, f1_score = 0.75\n",
      "Testing metrics for model_5: iou_score = 0.55, accuracy = 0.99, precision = 0.87, recall = 0.59, f1_score = 0.71\n",
      "Testing metrics for model_6: iou_score = 0.59, accuracy = 0.99, precision = 0.77, recall = 0.72, f1_score = 0.75\n",
      "Testing metrics for model_7: iou_score = 0.56, accuracy = 0.99, precision = 0.84, recall = 0.63, f1_score = 0.72\n",
      "Testing metrics for model_8: iou_score = 0.60, accuracy = 0.99, precision = 0.81, recall = 0.70, f1_score = 0.75\n",
      "Testing metrics for model_9: iou_score = 0.59, accuracy = 0.99, precision = 0.74, recall = 0.75, f1_score = 0.74\n",
      "Testing metrics for model_10: iou_score = 0.59, accuracy = 0.99, precision = 0.80, recall = 0.70, f1_score = 0.74\n"
     ]
    }
   ],
   "source": [
    "test(net, idx, params, data, current_round=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "al_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
