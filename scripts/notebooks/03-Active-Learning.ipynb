{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d41c6ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "from models import *\n",
    "from strategies import *\n",
    "from custom_datasets import *\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "import tqdm\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "import segmentation_models_pytorch as smp\n",
    "import os\n",
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "import torch.nn as nn\n",
    "\n",
    "from PIL import Image\n",
    "import torchvision.transforms as T\n",
    "import torchvision.transforms.functional as TF\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "\n",
    "import time\n",
    "import json\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a6e9cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_path = \"/root/Master_Thesis/\"\n",
    "dataframes_path = main_path + \"data/dataframes/\"\n",
    "sam_path = main_path + \"sam/sam_vit_h_4b8939.pth\"\n",
    "expirements_path = main_path+\"expirements/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad6925f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_name = \"brain_df\"\n",
    "# train_df = pd.read_csv(dataframes_path+\"brain_df_train.csv\")\n",
    "# test_df = pd.read_csv(dataframes_path+\"brain_df_test.csv\")\n",
    "\n",
    "## Too misclassifications\n",
    "# df_name = \"fire_df\"\n",
    "# train_df = pd.read_csv(dataframes_path+\"fire_df_train.csv\")\n",
    "# test_df = pd.read_csv(dataframes_path+\"fire_df_test.csv\")\n",
    "\n",
    "# Couldn't learn from it\n",
    "# df_name = \"aerial_df\"\n",
    "# train_df = pd.read_csv(dataframes_path+\"aerial_df_train.csv\")\n",
    "# test_df = pd.read_csv(dataframes_path+\"aerial_df_test.csv\")\n",
    "\n",
    "df_name = \"lung_df\"\n",
    "train_df = pd.read_csv(dataframes_path+\"lung_df_train.csv\")\n",
    "test_df = pd.read_csv(dataframes_path+\"lung_df_test.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39273399",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4731"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0780622c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_epoch': 20, 'train_args': {'batch_size': 128, 'num_workers': 1}, 'test_args': {'batch_size': 256, 'num_workers': 1}, 'optimizer_args': {'lr': 0.005, 'momentum': 0.9}, 'use_sam': False, 'use_predictor': False, 'use_generator': False, 'init_set_size': 11036, 'query_num': 4, 'rounds': 1, 'activate_sam_at_round': 4, 'test_set_size': 4731, 'df': 'lung_df'}\n"
     ]
    }
   ],
   "source": [
    "params = {'n_epoch': 20,\n",
    "          'train_args':{'batch_size': 128, 'num_workers': 1},\n",
    "          'test_args':{'batch_size': 256, 'num_workers': 1},\n",
    "          'optimizer_args':{'lr': 0.00005, 'momentum': 0.9},\n",
    "          'use_sam': False,\n",
    "          'use_predictor': False,\n",
    "          'use_generator': False,\n",
    "          'init_set_size': len(train_df),#20,\n",
    "          'query_num': 4, #int(0.1*len(test_df)),\n",
    "          'rounds': 1,\n",
    "          \"activate_sam_at_round\":4, \n",
    "          'test_set_size': len(test_df),\n",
    "          'df': df_name}\n",
    "print(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6720b8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if params['use_sam']:\n",
    "    sam = SAMOracle(checkpoint_path=sam_path)\n",
    "else:\n",
    "    sam =None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b0b555b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = smp.create_model(\n",
    "            'FPN', encoder_name='resnet34', in_channels=3, classes = 1\n",
    "        )\n",
    "torch.save(model.state_dict(), 'init_state.pt')\n",
    "init_state = torch.load('init_state.pt')\n",
    "net = Net(model, params, device = torch.device(\"cuda:1\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "311a1574",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(handler, train_df, test_df):\n",
    "    # raw_train = AL_Seg_dataset(main_path + \"/data/processed/oracle/\", inp_df=train_df, init=True, transform=True, use_sam=params['use_sam'])\n",
    "    # raw_test = AL_Seg_dataset(main_path + \"/data/processed/oracle/\", inp_df=test_df, init=True, transform=True, use_sam=params['use_sam'])\n",
    "    # df = raw_train.df\n",
    "    return Data(train_df[\"images\"].to_list(), train_df[\"masks\"].to_list(), test_df[\"images\"].to_list(), test_df[\"masks\"].to_list(), handler, df=train_df, path= main_path+\"/data/processed/\", use_sam=params['use_sam'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0dfea906",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_data(Handler, train_df, test_df)\n",
    "data.initialize_labels(params[\"init_set_size\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f71ae3",
   "metadata": {},
   "source": [
    "### Choose an AL strategy from a)RandomSampling b)MarginSampling c)EntropySampling d)KCenterGreedy e)AdversarialBIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cc499e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy = MarginSampling(dataset=data, net=net, sam=sam)\n",
    "strategy.net.net.load_state_dict(init_state)\n",
    "params[\"strategy\"] = \"MarginSampling\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "79169b74",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 0\n",
      "11036\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|██████████████████████▉                            | 9/20 [40:15<49:11, 268.35s/it, loss=0.984]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m logs\u001b[38;5;241m=\u001b[39m[]\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRound 0\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m \u001b[43mstrategy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m logits, mask_gt \u001b[38;5;241m=\u001b[39m strategy\u001b[38;5;241m.\u001b[39mpredict(data\u001b[38;5;241m.\u001b[39mget_test_data())\n\u001b[1;32m      6\u001b[0m iou_score, accuracy, precision, recall, f1_score \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mcal_test_metrics(logits, mask_gt )\n",
      "File \u001b[0;32m~/Master_Thesis/scripts/notebooks/../src/strategies.py:244\u001b[0m, in \u001b[0;36mStrategy.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    241\u001b[0m \u001b[38;5;124;03mTrains the network using the labeled data in the dataset.\u001b[39;00m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    243\u001b[0m labeled_idxs, labeled_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39mget_labeled_data()\n\u001b[0;32m--> 244\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabeled_data\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Master_Thesis/scripts/notebooks/../src/models.py:77\u001b[0m, in \u001b[0;36mNet.train\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     75\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     76\u001b[0m \u001b[38;5;66;03m# prog_bar.set_postfix({\"Dice_loss\": loss.item()})\u001b[39;00m\n\u001b[0;32m---> 77\u001b[0m prog_bar\u001b[38;5;241m.\u001b[39mset_postfix(loss \u001b[38;5;241m=\u001b[39m  \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "logs=[]\n",
    "print(\"Round 0\")\n",
    "strategy.train()\n",
    "logits, mask_gt = strategy.predict(data.get_test_data())\n",
    "iou_score, accuracy, precision, recall, f1_score = data.cal_test_metrics(logits, mask_gt )\n",
    "logs.append(f\"Round 0 testing metrics: iou_score = {iou_score:.2f}, accuracy = {accuracy:.2f}, precision = {precision:.2f}, recall = {recall:.2f}, f1_score = {f1_score:.2f}\")\n",
    "print(logs[0])\n",
    "\n",
    "for rd in range(1, params[\"rounds\"]):\n",
    "    print(f\"Round {rd}\")\n",
    "\n",
    "    # query\n",
    "    print(\"Querying\")\n",
    "    query_idxs = strategy.query(params[\"query_num\"])\n",
    "    print(query_idxs)\n",
    "\n",
    "    # update labels\n",
    "    if params[\"use_sam\"] and rd >= params[\"activate_sam_at_round\"]:\n",
    "        print(\"Updating with sam\")\n",
    "        strategy.update(query_idxs, start_sam=True, use_predictor=params[\"use_predictor\"], use_generator=params[\"use_generator\"])\n",
    "        print(\"Sam failed to mask: \", strategy.sam_failed)\n",
    "    else:\n",
    "        print(\"Updating without sam\")\n",
    "        strategy.update(query_idxs)\n",
    "    \n",
    "    print(\"Reset and train\")\n",
    "    init_state = torch.load('init_state.pt')\n",
    "    strategy.net.net.load_state_dict(init_state)\n",
    "    strategy.train()\n",
    "\n",
    "    # calculate accuracy\n",
    "    logits, maks_gt = strategy.predict(data.get_test_data())\n",
    "    iou_score, accuracy, precision, recall, f1_score = data.cal_test_metrics(logits, mask_gt )\n",
    "    # logs.append(f\"Round {rd} testing metrics: iou_score = {iou_score:.2f}, accuracy = {accuracy:.2f}, precision = {precision:.2f}, recall = {recall:.2f}, f1_score = {f1_score:.2f}, human_envolved = {strategy.human_envolved}\")\n",
    "    logs.append(f\"Round {rd} testing metrics: iou_score = {iou_score:.2f}, accuracy = {accuracy:.2f}, precision = {precision:.2f}, recall = {recall:.2f}, f1_score = {f1_score:.2f}, sam_failed = {strategy.human_envolved}\")\n",
    "    strategy.human_envolved = 0\n",
    "    print(logs[rd])\n",
    "    \n",
    "params['logs'] = logs\n",
    "\n",
    "for dirname, _, filenames in os.walk(expirements_path):\n",
    "    filename = \"expirement_{}.json\".format(len(filenames))\n",
    "    file_path = os.path.join(dirname, filename)\n",
    "    with open(file_path, 'w') as f:\n",
    "        json.dump(params, f)\n",
    "        print(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97b9231",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = get_data(Handler, train_df, test_df)\n",
    "# data.initialize_labels(params[\"init_set_size\"])\n",
    "# strategy = EntropySampling(dataset=data, net=net, sam=sam)\n",
    "# strategy.net.net.load_state_dict(init_state)\n",
    "# params[\"strategy\"] = \"EntropySampling\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f26ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cuda.empty_cache()\n",
    "# logs=[]\n",
    "# print(\"Round 0\")\n",
    "# strategy.train()\n",
    "# logits, mask_gt = strategy.predict(data.get_test_data())\n",
    "# iou_score, accuracy, precision, recall, f1_score = data.cal_test_metrics(logits, mask_gt )\n",
    "# logs.append(f\"Round 0 testing metrics: iou_score = {iou_score:.2f}, accuracy = {accuracy:.2f}, precision = {precision:.2f}, recall = {recall:.2f}, f1_score = {f1_score:.2f}\")\n",
    "# print(logs[0])\n",
    "\n",
    "# for rd in range(1, params[\"rounds\"]):\n",
    "#     print(f\"Round {rd}\")\n",
    "\n",
    "#     # query\n",
    "#     print(\"Querying\")\n",
    "#     query_idxs = strategy.query(params[\"query_num\"])\n",
    "#     print(query_idxs)\n",
    "\n",
    "#     # update labels\n",
    "#     if params[\"use_sam\"] and rd >= params[\"activate_sam_at_round\"]:\n",
    "#         print(\"Updating with sam\")\n",
    "#         strategy.update(query_idxs, start_sam=True, use_predictor=params[\"use_predictor\"], use_generator=params[\"use_generator\"])\n",
    "#     else:\n",
    "#         print(\"Updating without sam\")\n",
    "#         strategy.update(query_idxs)\n",
    "    \n",
    "#     print(\"Reset and train\")\n",
    "#     init_state = torch.load('init_state.pt')\n",
    "#     strategy.net.net.load_state_dict(init_state)\n",
    "#     strategy.train()\n",
    "\n",
    "#     # calculate accuracy\n",
    "#     logits, maks_gt = strategy.predict(data.get_test_data())\n",
    "#     iou_score, accuracy, precision, recall, f1_score = data.cal_test_metrics(logits, mask_gt )\n",
    "#     logs.append(f\"Round {rd} testing metrics: iou_score = {iou_score:.2f}, accuracy = {accuracy:.2f}, precision = {precision:.2f}, recall = {recall:.2f}, f1_score = {f1_score:.2f}\")\n",
    "#     print(logs[rd])\n",
    "    \n",
    "# params['logs'] = logs\n",
    "\n",
    "# for dirname, _, filenames in os.walk(expirements_path):\n",
    "#     filename = \"expirement_{}.json\".format(len(filenames))\n",
    "#     file_path = os.path.join(dirname, filename)\n",
    "#     with open(file_path, 'w') as f:\n",
    "#         json.dump(params, f)\n",
    "#         print(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9430928",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = get_data(Handler, train_df, test_df)\n",
    "# data.initialize_labels(params[\"init_set_size\"])\n",
    "# strategy = BALDDropout(dataset=data, net=net, sam=sam)\n",
    "# strategy.net.net.load_state_dict(init_state)\n",
    "# params[\"strategy\"] = \"BALDDropout\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6dcca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cuda.empty_cache()\n",
    "# logs=[]\n",
    "# print(\"Round 0\")\n",
    "# strategy.train()\n",
    "# logits, mask_gt = strategy.predict(data.get_test_data())\n",
    "# iou_score, accuracy, precision, recall, f1_score = data.cal_test_metrics(logits, mask_gt )\n",
    "# logs.append(f\"Round 0 testing metrics: iou_score = {iou_score:.2f}, accuracy = {accuracy:.2f}, precision = {precision:.2f}, recall = {recall:.2f}, f1_score = {f1_score:.2f}\")\n",
    "# print(logs[0])\n",
    "\n",
    "# for rd in range(1, params[\"rounds\"]):\n",
    "#     print(f\"Round {rd}\")\n",
    "\n",
    "#     # query\n",
    "#     print(\"Querying\")\n",
    "#     query_idxs = strategy.query(params[\"query_num\"])\n",
    "#     print(query_idxs)\n",
    "\n",
    "#     # update labels\n",
    "#     if params[\"use_sam\"] and rd >= params[\"activate_sam_at_round\"]:\n",
    "#         print(\"Updating with sam\")\n",
    "#         strategy.update(query_idxs, start_sam=True, use_predictor=params[\"use_predictor\"], use_generator=params[\"use_generator\"])\n",
    "#     else:\n",
    "#         print(\"Updating without sam\")\n",
    "#         strategy.update(query_idxs)\n",
    "    \n",
    "#     print(\"Reset and train\")\n",
    "#     init_state = torch.load('init_state.pt')\n",
    "#     strategy.net.net.load_state_dict(init_state)\n",
    "#     strategy.train()\n",
    "\n",
    "#     # calculate accuracy\n",
    "#     logits, maks_gt = strategy.predict(data.get_test_data())\n",
    "#     iou_score, accuracy, precision, recall, f1_score = data.cal_test_metrics(logits, mask_gt )\n",
    "#     logs.append(f\"Round {rd} testing metrics: iou_score = {iou_score:.2f}, accuracy = {accuracy:.2f}, precision = {precision:.2f}, recall = {recall:.2f}, f1_score = {f1_score:.2f}\")\n",
    "#     print(logs[rd])\n",
    "    \n",
    "# params['logs'] = logs\n",
    "\n",
    "# for dirname, _, filenames in os.walk(expirements_path):\n",
    "#     filename = \"expirement_{}.json\".format(len(filenames))\n",
    "#     file_path = os.path.join(dirname, filename)\n",
    "#     with open(file_path, 'w') as f:\n",
    "#         json.dump(params, f)\n",
    "#         print(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c7703b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = get_data(Handler, train_df, test_df)\n",
    "# data.initialize_labels(params[\"init_set_size\"])\n",
    "# strategy = AdversarialBIM(dataset=data, net=net, sam=sam)\n",
    "# strategy.net.net.load_state_dict(init_state)\n",
    "# params[\"strategy\"] = \"AdversarialBIM\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998c1e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cuda.empty_cache()\n",
    "# logs=[]\n",
    "# print(\"Round 0\")\n",
    "# strategy.train()\n",
    "# logits, mask_gt = strategy.predict(data.get_test_data())\n",
    "# iou_score, accuracy, precision, recall, f1_score = data.cal_test_metrics(logits, mask_gt )\n",
    "# logs.append(f\"Round 0 testing metrics: iou_score = {iou_score:.2f}, accuracy = {accuracy:.2f}, precision = {precision:.2f}, recall = {recall:.2f}, f1_score = {f1_score:.2f}\")\n",
    "# print(logs[0])\n",
    "\n",
    "# for rd in range(1, params[\"rounds\"]):\n",
    "#     print(f\"Round {rd}\")\n",
    "\n",
    "#     # query\n",
    "#     print(\"Querying\")\n",
    "#     query_idxs = strategy.query(params[\"query_num\"])\n",
    "#     print(query_idxs)\n",
    "\n",
    "#     # update labels\n",
    "#     if params[\"use_sam\"] and rd >= params[\"activate_sam_at_round\"]:\n",
    "#         print(\"Updating with sam\")\n",
    "#         strategy.update(query_idxs, start_sam=True, use_predictor=params[\"use_predictor\"], use_generator=params[\"use_generator\"])\n",
    "#     else:\n",
    "#         print(\"Updating without sam\")\n",
    "#         strategy.update(query_idxs)\n",
    "    \n",
    "#     print(\"Reset and train\")\n",
    "#     init_state = torch.load('init_state.pt')\n",
    "#     strategy.net.net.load_state_dict(init_state)\n",
    "#     strategy.train()\n",
    "\n",
    "#     # calculate accuracy\n",
    "#     logits, maks_gt = strategy.predict(data.get_test_data())\n",
    "#     iou_score, accuracy, precision, recall, f1_score = data.cal_test_metrics(logits, mask_gt )\n",
    "#     logs.append(f\"Round {rd} testing metrics: iou_score = {iou_score:.2f}, accuracy = {accuracy:.2f}, precision = {precision:.2f}, recall = {recall:.2f}, f1_score = {f1_score:.2f}\")\n",
    "#     print(logs[rd])\n",
    "    \n",
    "# params['logs'] = logs\n",
    "\n",
    "# for dirname, _, filenames in os.walk(expirements_path):\n",
    "#     filename = \"expirement_{}.json\".format(len(filenames))\n",
    "#     file_path = os.path.join(dirname, filename)\n",
    "#     with open(file_path, 'w') as f:\n",
    "#         json.dump(params, f)\n",
    "#         print(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbab36a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = get_data(Handler, train_df, test_df)\n",
    "# data.initialize_labels(params[\"init_set_size\"])\n",
    "# strategy = KCenterGreedy(dataset=data, net=net, sam=sam)\n",
    "# strategy.net.net.load_state_dict(init_state)\n",
    "# params[\"strategy\"] = \"KCenterGreedy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6d7a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cuda.empty_cache()\n",
    "# logs=[]\n",
    "# print(\"Round 0\")\n",
    "# strategy.train()\n",
    "# logits, mask_gt = strategy.predict(data.get_test_data())\n",
    "# iou_score, accuracy, precision, recall, f1_score = data.cal_test_metrics(logits, mask_gt )\n",
    "# logs.append(f\"Round 0 testing metrics: iou_score = {iou_score:.2f}, accuracy = {accuracy:.2f}, precision = {precision:.2f}, recall = {recall:.2f}, f1_score = {f1_score:.2f}\")\n",
    "# print(logs[0])\n",
    "\n",
    "# for rd in range(1, params[\"rounds\"]):\n",
    "#     print(f\"Round {rd}\")\n",
    "\n",
    "#     # query\n",
    "#     print(\"Querying\")\n",
    "#     query_idxs = strategy.query(params[\"query_num\"])\n",
    "#     print(query_idxs)\n",
    "\n",
    "#     # update labels\n",
    "#     if params[\"use_sam\"] and rd >= params[\"activate_sam_at_round\"]:\n",
    "#         print(\"Updating with sam\")\n",
    "#         strategy.update(query_idxs, start_sam=True, use_predictor=params[\"use_predictor\"], use_generator=params[\"use_generator\"])\n",
    "#     else:\n",
    "#         print(\"Updating without sam\")\n",
    "#         strategy.update(query_idxs)\n",
    "    \n",
    "#     print(\"Reset and train\")\n",
    "#     init_state = torch.load('init_state.pt')\n",
    "#     strategy.net.net.load_state_dict(init_state)\n",
    "#     strategy.train()\n",
    "\n",
    "#     # calculate accuracy\n",
    "#     logits, maks_gt = strategy.predict(data.get_test_data())\n",
    "#     iou_score, accuracy, precision, recall, f1_score = data.cal_test_metrics(logits, mask_gt )\n",
    "#     logs.append(f\"Round {rd} testing metrics: iou_score = {iou_score:.2f}, accuracy = {accuracy:.2f}, precision = {precision:.2f}, recall = {recall:.2f}, f1_score = {f1_score:.2f}\")\n",
    "#     print(logs[rd])\n",
    "    \n",
    "# params['logs'] = logs\n",
    "\n",
    "# for dirname, _, filenames in os.walk(expirements_path):\n",
    "#     filename = \"expirement_{}.json\".format(len(filenames))\n",
    "#     file_path = os.path.join(dirname, filename)\n",
    "#     with open(file_path, 'w') as f:\n",
    "#         json.dump(params, f)\n",
    "#         print(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e0141c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
