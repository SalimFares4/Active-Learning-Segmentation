{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d41c6ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "from models import *\n",
    "from strategies import *\n",
    "from custom_datasets import *\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "import tqdm\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "import segmentation_models_pytorch as smp\n",
    "import os\n",
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "import torch.nn as nn\n",
    "\n",
    "from PIL import Image\n",
    "import torchvision.transforms as T\n",
    "import torchvision.transforms.functional as TF\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "\n",
    "import time\n",
    "import json\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a6e9cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_path = \"/root/Master_Thesis/\"\n",
    "dataframes_path = main_path + \"data/dataframes/\"\n",
    "sam_path = main_path + \"sam/sam_vit_h_4b8939.pth\"\n",
    "expirements_path = main_path+\"expirements/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad6925f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_name = \"brain_df\"\n",
    "train_df = pd.read_csv(dataframes_path+\"brain_df_train.csv\")\n",
    "test_df = pd.read_csv(dataframes_path+\"brain_df_test.csv\")\n",
    "\n",
    "## Too misclassifications\n",
    "# df_name = \"fire_df\"\n",
    "# train_df = pd.read_csv(dataframes_path+\"fire_df_train.csv\")\n",
    "# test_df = pd.read_csv(dataframes_path+\"fire_df_test.csv\")\n",
    "\n",
    "# Couldn't learn from it\n",
    "# df_name = \"aerial_df\"\n",
    "# train_df = pd.read_csv(dataframes_path+\"aerial_df_train.csv\")\n",
    "# test_df = pd.read_csv(dataframes_path+\"aerial_df_test.csv\")\n",
    "\n",
    "# Couldn't learn from it\n",
    "# df_name = \"lung_df\"\n",
    "# train_df = pd.read_csv(dataframes_path+\"lung_df_train.csv\")\n",
    "# test_df = pd.read_csv(dataframes_path+\"lung_df_test.csv\")\n",
    "\n",
    "# Couldn't learn from it\n",
    "# df_name = \"lung_tumor_df\"\n",
    "# train_df = pd.read_csv(dataframes_path+\"lung_tumor_df_train.csv\")\n",
    "# test_df = pd.read_csv(dataframes_path+\"lung_tumor_df_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39273399",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1179"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0780622c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_epoch': 25, 'train_args': {'batch_size': 4, 'num_workers': 1}, 'test_args': {'batch_size': 256, 'num_workers': 1}, 'optimizer_args': {'lr': 0.005, 'momentum': 0.9}, 'use_sam': False, 'use_predictor': False, 'use_generator': False, 'init_set_size': 100, 'query_num': 5, 'rounds': 2, 'activate_sam_at_round': 1, 'test_set_size': 1179, 'df': 'brain_df'}\n"
     ]
    }
   ],
   "source": [
    "params = {'n_epoch': 25,\n",
    "          'train_args':{'batch_size': 4, 'num_workers': 1},\n",
    "          'test_args':{'batch_size': 256, 'num_workers': 1},\n",
    "          'optimizer_args':{'lr': 5e-3, 'momentum': 0.9},\n",
    "          'use_sam': False,\n",
    "          'use_predictor': False,\n",
    "          'use_generator': False,\n",
    "          'init_set_size': 100,\n",
    "          'query_num': 5, #int(0.1*len(test_df)),\n",
    "          'rounds': 2,\n",
    "          \"activate_sam_at_round\":1, \n",
    "          'test_set_size': len(test_df),\n",
    "          'df': df_name,\n",
    "          \"img_size\": (256, 256)}\n",
    "\n",
    "print(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6720b8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if params['use_sam']:\n",
    "    sam = SAMOracle(checkpoint_path=sam_path, img_size=params[\"img_size\"])\n",
    "else:\n",
    "    sam =None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b0b555b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = smp.create_model(\n",
    "            'Unet', encoder_name='resnet34', in_channels=3, classes = 1\n",
    "        )\n",
    "# torch.save(model.state_dict(), 'init_state.pt')\n",
    "init_state = torch.load('init_state_Unet.pt')\n",
    "# net = Net(model, params, device = torch.device(\"cuda:1\"))\n",
    "net = Net(model, params, device = torch.device(\"cuda\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "311a1574",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(handler, train_df, test_df):\n",
    "    # raw_train = AL_Seg_dataset(main_path + \"/data/processed/oracle/\", inp_df=train_df, init=True, transform=True, use_sam=params['use_sam'])\n",
    "    # raw_test = AL_Seg_dataset(main_path + \"/data/processed/oracle/\", inp_df=test_df, init=True, transform=True, use_sam=params['use_sam'])\n",
    "    # df = raw_train.df\n",
    "    return Data(train_df[\"images\"].to_list(), train_df[\"masks\"].to_list(), test_df[\"images\"].to_list(), test_df[\"masks\"].to_list(), handler, img_size=params[\"img_size\"], df=train_df, path= main_path+\"/data/processed/\", use_sam=params['use_sam'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0dfea906",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_data(Handler, train_df, test_df)\n",
    "data.initialize_labels(params[\"init_set_size\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f71ae3",
   "metadata": {},
   "source": [
    "### Choose an AL strategy from a)RandomSampling b)MarginSampling c)EntropySampling d)KCenterGreedy e)AdversarialBIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cc499e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy = MarginSampling(dataset=data, net=net, sam=sam)\n",
    "strategy.net.net.load_state_dict(init_state)\n",
    "params[\"strategy\"] = \"MarginSampling\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "79169b74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 0\n",
      "100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████| 25/25 [02:49<00:00,  6.79s/it, loss=0.275]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 0 testing metrics: iou_score = 0.24, accuracy = 0.99, precision = 0.76, recall = 0.26, f1_score = 0.39\n",
      "Model's saved!\n",
      "\n",
      "Round 1\n",
      "Querying\n",
      "[1233 1271 1266 1256 1249]\n",
      "Updating with sam\n",
      "Sam failed to mask:  []\n",
      "Reset and train\n",
      "105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████| 25/25 [03:02<00:00,  7.30s/it, loss=0.127]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 1 testing metrics: iou_score = 0.52, accuracy = 0.99, precision = 0.81, recall = 0.60, f1_score = 0.69, sam_failed = 0\n",
      "expirement_37.json\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "logs=[]\n",
    "print(\"Round 0\")\n",
    "strategy.train()\n",
    "logits, mask_gt = strategy.predict(data.get_test_data())\n",
    "iou_score, accuracy, precision, recall, f1_score = data.cal_test_metrics(logits, mask_gt )\n",
    "logs.append(f\"Round 0 testing metrics: iou_score = {iou_score:.2f}, accuracy = {accuracy:.2f}, precision = {precision:.2f}, recall = {recall:.2f}, f1_score = {f1_score:.2f}\")\n",
    "print(logs[0])\n",
    "\n",
    "torch.save( strategy.net.net.state_dict(), 'trained_before_sam.pt')\n",
    "print(\"Model's saved!\\n\")\n",
    "\n",
    "for rd in range(1, params[\"rounds\"]):\n",
    "    print(f\"Round {rd}\")\n",
    "\n",
    "    # query\n",
    "    print(\"Querying\")\n",
    "    query_idxs = strategy.query(params[\"query_num\"])\n",
    "    print(query_idxs)\n",
    "\n",
    "    # update labels\n",
    "    if params[\"use_sam\"] and rd >= params[\"activate_sam_at_round\"]:\n",
    "        print(\"Updating with sam\")\n",
    "        strategy.update(query_idxs, start_sam=True, use_predictor=params[\"use_predictor\"], use_generator=params[\"use_generator\"])\n",
    "        print(\"Sam failed to mask: \", strategy.sam_failed)\n",
    "    else:\n",
    "        print(\"Updating without sam\")\n",
    "        strategy.update(query_idxs)\n",
    "    \n",
    "    print(\"Reset and train\")\n",
    "    init_state = torch.load('init_state_Unet.pt')\n",
    "    strategy.net.net.load_state_dict(init_state)\n",
    "    strategy.train()\n",
    "\n",
    "    # calculate accuracy\n",
    "    logits, maks_gt = strategy.predict(data.get_test_data())\n",
    "    iou_score, accuracy, precision, recall, f1_score = data.cal_test_metrics(logits, mask_gt )\n",
    "    # logs.append(f\"Round {rd} testing metrics: iou_score = {iou_score:.2f}, accuracy = {accuracy:.2f}, precision = {precision:.2f}, recall = {recall:.2f}, f1_score = {f1_score:.2f}, human_envolved = {strategy.human_envolved}\")\n",
    "    logs.append(f\"Round {rd} testing metrics: iou_score = {iou_score:.2f}, accuracy = {accuracy:.2f}, precision = {precision:.2f}, recall = {recall:.2f}, f1_score = {f1_score:.2f}, sam_failed = {strategy.human_envolved}\")\n",
    "    strategy.human_envolved = 0\n",
    "    print(logs[rd])\n",
    "    \n",
    "params['logs'] = logs\n",
    "\n",
    "for dirname, _, filenames in os.walk(expirements_path):\n",
    "    filename = \"expirement_{}.json\".format(len(filenames))\n",
    "    file_path = os.path.join(dirname, filename)\n",
    "    with open(file_path, 'w') as f:\n",
    "        json.dump(params, f)\n",
    "        print(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b97b9231",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = get_data(Handler, train_df, test_df)\n",
    "# data.initialize_labels(params[\"init_set_size\"])\n",
    "# strategy = EntropySampling(dataset=data, net=net, sam=sam)\n",
    "# strategy.net.net.load_state_dict(init_state)\n",
    "# params[\"strategy\"] = \"EntropySampling\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b8f26ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cuda.empty_cache()\n",
    "# logs=[]\n",
    "# print(\"Round 0\")\n",
    "# strategy.train()\n",
    "# logits, mask_gt = strategy.predict(data.get_test_data())\n",
    "# iou_score, accuracy, precision, recall, f1_score = data.cal_test_metrics(logits, mask_gt )\n",
    "# logs.append(f\"Round 0 testing metrics: iou_score = {iou_score:.2f}, accuracy = {accuracy:.2f}, precision = {precision:.2f}, recall = {recall:.2f}, f1_score = {f1_score:.2f}\")\n",
    "# print(logs[0])\n",
    "\n",
    "# for rd in range(1, params[\"rounds\"]):\n",
    "#     print(f\"Round {rd}\")\n",
    "\n",
    "#     # query\n",
    "#     print(\"Querying\")\n",
    "#     query_idxs = strategy.query(params[\"query_num\"])\n",
    "#     print(query_idxs)\n",
    "\n",
    "#     # update labels\n",
    "#     if params[\"use_sam\"] and rd >= params[\"activate_sam_at_round\"]:\n",
    "#         print(\"Updating with sam\")\n",
    "#         strategy.update(query_idxs, start_sam=True, use_predictor=params[\"use_predictor\"], use_generator=params[\"use_generator\"])\n",
    "#     else:\n",
    "#         print(\"Updating without sam\")\n",
    "#         strategy.update(query_idxs)\n",
    "    \n",
    "#     print(\"Reset and train\")\n",
    "#     init_state = torch.load('init_state.pt')\n",
    "#     strategy.net.net.load_state_dict(init_state)\n",
    "#     strategy.train()\n",
    "\n",
    "#     # calculate accuracy\n",
    "#     logits, maks_gt = strategy.predict(data.get_test_data())\n",
    "#     iou_score, accuracy, precision, recall, f1_score = data.cal_test_metrics(logits, mask_gt )\n",
    "#     logs.append(f\"Round {rd} testing metrics: iou_score = {iou_score:.2f}, accuracy = {accuracy:.2f}, precision = {precision:.2f}, recall = {recall:.2f}, f1_score = {f1_score:.2f}\")\n",
    "#     print(logs[rd])\n",
    "    \n",
    "# params['logs'] = logs\n",
    "\n",
    "# for dirname, _, filenames in os.walk(expirements_path):\n",
    "#     filename = \"expirement_{}.json\".format(len(filenames))\n",
    "#     file_path = os.path.join(dirname, filename)\n",
    "#     with open(file_path, 'w') as f:\n",
    "#         json.dump(params, f)\n",
    "#         print(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d9430928",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = get_data(Handler, train_df, test_df)\n",
    "# data.initialize_labels(params[\"init_set_size\"])\n",
    "# strategy = BALDDropout(dataset=data, net=net, sam=sam)\n",
    "# strategy.net.net.load_state_dict(init_state)\n",
    "# params[\"strategy\"] = \"BALDDropout\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0e6dcca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cuda.empty_cache()\n",
    "# logs=[]\n",
    "# print(\"Round 0\")\n",
    "# strategy.train()\n",
    "# logits, mask_gt = strategy.predict(data.get_test_data())\n",
    "# iou_score, accuracy, precision, recall, f1_score = data.cal_test_metrics(logits, mask_gt )\n",
    "# logs.append(f\"Round 0 testing metrics: iou_score = {iou_score:.2f}, accuracy = {accuracy:.2f}, precision = {precision:.2f}, recall = {recall:.2f}, f1_score = {f1_score:.2f}\")\n",
    "# print(logs[0])\n",
    "\n",
    "# for rd in range(1, params[\"rounds\"]):\n",
    "#     print(f\"Round {rd}\")\n",
    "\n",
    "#     # query\n",
    "#     print(\"Querying\")\n",
    "#     query_idxs = strategy.query(params[\"query_num\"])\n",
    "#     print(query_idxs)\n",
    "\n",
    "#     # update labels\n",
    "#     if params[\"use_sam\"] and rd >= params[\"activate_sam_at_round\"]:\n",
    "#         print(\"Updating with sam\")\n",
    "#         strategy.update(query_idxs, start_sam=True, use_predictor=params[\"use_predictor\"], use_generator=params[\"use_generator\"])\n",
    "#     else:\n",
    "#         print(\"Updating without sam\")\n",
    "#         strategy.update(query_idxs)\n",
    "    \n",
    "#     print(\"Reset and train\")\n",
    "#     init_state = torch.load('init_state.pt')\n",
    "#     strategy.net.net.load_state_dict(init_state)\n",
    "#     strategy.train()\n",
    "\n",
    "#     # calculate accuracy\n",
    "#     logits, maks_gt = strategy.predict(data.get_test_data())\n",
    "#     iou_score, accuracy, precision, recall, f1_score = data.cal_test_metrics(logits, mask_gt )\n",
    "#     logs.append(f\"Round {rd} testing metrics: iou_score = {iou_score:.2f}, accuracy = {accuracy:.2f}, precision = {precision:.2f}, recall = {recall:.2f}, f1_score = {f1_score:.2f}\")\n",
    "#     print(logs[rd])\n",
    "    \n",
    "# params['logs'] = logs\n",
    "\n",
    "# for dirname, _, filenames in os.walk(expirements_path):\n",
    "#     filename = \"expirement_{}.json\".format(len(filenames))\n",
    "#     file_path = os.path.join(dirname, filename)\n",
    "#     with open(file_path, 'w') as f:\n",
    "#         json.dump(params, f)\n",
    "#         print(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "62c7703b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = get_data(Handler, train_df, test_df)\n",
    "# data.initialize_labels(params[\"init_set_size\"])\n",
    "# strategy = AdversarialBIM(dataset=data, net=net, sam=sam)\n",
    "# strategy.net.net.load_state_dict(init_state)\n",
    "# params[\"strategy\"] = \"AdversarialBIM\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "998c1e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cuda.empty_cache()\n",
    "# logs=[]\n",
    "# print(\"Round 0\")\n",
    "# strategy.train()\n",
    "# logits, mask_gt = strategy.predict(data.get_test_data())\n",
    "# iou_score, accuracy, precision, recall, f1_score = data.cal_test_metrics(logits, mask_gt )\n",
    "# logs.append(f\"Round 0 testing metrics: iou_score = {iou_score:.2f}, accuracy = {accuracy:.2f}, precision = {precision:.2f}, recall = {recall:.2f}, f1_score = {f1_score:.2f}\")\n",
    "# print(logs[0])\n",
    "\n",
    "# for rd in range(1, params[\"rounds\"]):\n",
    "#     print(f\"Round {rd}\")\n",
    "\n",
    "#     # query\n",
    "#     print(\"Querying\")\n",
    "#     query_idxs = strategy.query(params[\"query_num\"])\n",
    "#     print(query_idxs)\n",
    "\n",
    "#     # update labels\n",
    "#     if params[\"use_sam\"] and rd >= params[\"activate_sam_at_round\"]:\n",
    "#         print(\"Updating with sam\")\n",
    "#         strategy.update(query_idxs, start_sam=True, use_predictor=params[\"use_predictor\"], use_generator=params[\"use_generator\"])\n",
    "#     else:\n",
    "#         print(\"Updating without sam\")\n",
    "#         strategy.update(query_idxs)\n",
    "    \n",
    "#     print(\"Reset and train\")\n",
    "#     init_state = torch.load('init_state.pt')\n",
    "#     strategy.net.net.load_state_dict(init_state)\n",
    "#     strategy.train()\n",
    "\n",
    "#     # calculate accuracy\n",
    "#     logits, maks_gt = strategy.predict(data.get_test_data())\n",
    "#     iou_score, accuracy, precision, recall, f1_score = data.cal_test_metrics(logits, mask_gt )\n",
    "#     logs.append(f\"Round {rd} testing metrics: iou_score = {iou_score:.2f}, accuracy = {accuracy:.2f}, precision = {precision:.2f}, recall = {recall:.2f}, f1_score = {f1_score:.2f}\")\n",
    "#     print(logs[rd])\n",
    "    \n",
    "# params['logs'] = logs\n",
    "\n",
    "# for dirname, _, filenames in os.walk(expirements_path):\n",
    "#     filename = \"expirement_{}.json\".format(len(filenames))\n",
    "#     file_path = os.path.join(dirname, filename)\n",
    "#     with open(file_path, 'w') as f:\n",
    "#         json.dump(params, f)\n",
    "#         print(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bbab36a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = get_data(Handler, train_df, test_df)\n",
    "# data.initialize_labels(params[\"init_set_size\"])\n",
    "# strategy = KCenterGreedy(dataset=data, net=net, sam=sam)\n",
    "# strategy.net.net.load_state_dict(init_state)\n",
    "# params[\"strategy\"] = \"KCenterGreedy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2c6d7a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cuda.empty_cache()\n",
    "# logs=[]\n",
    "# print(\"Round 0\")\n",
    "# strategy.train()\n",
    "# logits, mask_gt = strategy.predict(data.get_test_data())\n",
    "# iou_score, accuracy, precision, recall, f1_score = data.cal_test_metrics(logits, mask_gt )\n",
    "# logs.append(f\"Round 0 testing metrics: iou_score = {iou_score:.2f}, accuracy = {accuracy:.2f}, precision = {precision:.2f}, recall = {recall:.2f}, f1_score = {f1_score:.2f}\")\n",
    "# print(logs[0])\n",
    "\n",
    "# for rd in range(1, params[\"rounds\"]):\n",
    "#     print(f\"Round {rd}\")\n",
    "\n",
    "#     # query\n",
    "#     print(\"Querying\")\n",
    "#     query_idxs = strategy.query(params[\"query_num\"])\n",
    "#     print(query_idxs)\n",
    "\n",
    "#     # update labels\n",
    "#     if params[\"use_sam\"] and rd >= params[\"activate_sam_at_round\"]:\n",
    "#         print(\"Updating with sam\")\n",
    "#         strategy.update(query_idxs, start_sam=True, use_predictor=params[\"use_predictor\"], use_generator=params[\"use_generator\"])\n",
    "#     else:\n",
    "#         print(\"Updating without sam\")\n",
    "#         strategy.update(query_idxs)\n",
    "    \n",
    "#     print(\"Reset and train\")\n",
    "#     init_state = torch.load('init_state.pt')\n",
    "#     strategy.net.net.load_state_dict(init_state)\n",
    "#     strategy.train()\n",
    "\n",
    "#     # calculate accuracy\n",
    "#     logits, maks_gt = strategy.predict(data.get_test_data())\n",
    "#     iou_score, accuracy, precision, recall, f1_score = data.cal_test_metrics(logits, mask_gt )\n",
    "#     logs.append(f\"Round {rd} testing metrics: iou_score = {iou_score:.2f}, accuracy = {accuracy:.2f}, precision = {precision:.2f}, recall = {recall:.2f}, f1_score = {f1_score:.2f}\")\n",
    "#     print(logs[rd])\n",
    "    \n",
    "# params['logs'] = logs\n",
    "\n",
    "# for dirname, _, filenames in os.walk(expirements_path):\n",
    "#     filename = \"expirement_{}.json\".format(len(filenames))\n",
    "#     file_path = os.path.join(dirname, filename)\n",
    "#     with open(file_path, 'w') as f:\n",
    "#         json.dump(params, f)\n",
    "#         print(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e0141c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:myenv]",
   "language": "python",
   "name": "conda-env-myenv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
