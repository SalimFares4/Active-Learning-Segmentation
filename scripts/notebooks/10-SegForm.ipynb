{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4039bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q datasets transformers evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a7ba561",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f30b9b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds = load_dataset(\"scene_parse_150\", split=\"train[:50]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b191f066",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds[0][\"image\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d87ac447",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds = ds.train_test_split(test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa62341a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_ds = ds[\"train\"]\n",
    "# test_ds = ds[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2fec287f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from huggingface_hub import cached_download, hf_hub_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "34bbbc3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# repo_id = \"huggingface/label-files\"\n",
    "# filename = \"ade20k-id2label.json\"\n",
    "# id2label = json.load(open(cached_download(hf_hub_url(repo_id, filename, repo_type=\"dataset\")), \"r\"))\n",
    "# id2label = {int(k): v for k, v in id2label.items()}\n",
    "# label2id = {v: k for k, v in id2label.items()}\n",
    "# num_labels = len(id2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a2569930",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label={1: 'yes'}\n",
    "\n",
    "label2id = {'yes': 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "03491a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoImageProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c9487d13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n",
      "/opt/conda/envs/al_env/lib/python3.9/site-packages/transformers/models/segformer/image_processing_segformer.py:101: FutureWarning: The `reduce_labels` parameter is deprecated and will be removed in a future version. Please use `do_reduce_labels` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "checkpoint = \"nvidia/mit-b0\"\n",
    "image_processor = AutoImageProcessor.from_pretrained(checkpoint, reduce_labels=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9cf94043",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchvision.transforms import ColorJitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e42e75b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# jitter = ColorJitter(brightness=0.25, contrast=0.25, saturation=0.25, hue=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8ec9a160",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_transforms(example_batch):\n",
    "#     images = [jitter(x) for x in example_batch[\"image\"]]\n",
    "#     labels = [x for x in example_batch[\"annotation\"]]\n",
    "#     inputs = image_processor(images, labels)\n",
    "#     return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aad36e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def val_transforms(example_batch):\n",
    "#     images = [x for x in example_batch[\"image\"]]\n",
    "#     labels = [x for x in example_batch[\"annotation\"]]\n",
    "#     inputs = image_processor(images, labels)\n",
    "#     return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4de12397",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_ds.set_transform(train_transforms)\n",
    "# test_ds.set_transform(val_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3dbafc3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# type(train_ds[0]['pixel_values'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a72bef48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0a32ff0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = evaluate.load(\"mean_iou\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ce37a585",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c2bb0077",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    with torch.no_grad():\n",
    "        logits, labels = eval_pred\n",
    "        logits_tensor = torch.from_numpy(logits)\n",
    "        logits_tensor = nn.functional.interpolate(\n",
    "            logits_tensor,\n",
    "            size=labels.shape[-2:],\n",
    "            mode=\"bilinear\",\n",
    "            align_corners=False,\n",
    "        ).argmax(dim=1)\n",
    "\n",
    "        pred_labels = logits_tensor.detach().cpu().numpy()\n",
    "        metrics = metric.compute(\n",
    "            predictions=pred_labels,\n",
    "            references=labels,\n",
    "            num_labels=num_labels,\n",
    "            ignore_index=255,\n",
    "            reduce_labels=False,\n",
    "        )\n",
    "        for key, value in metrics.items():\n",
    "            if type(value) is np.ndarray:\n",
    "                metrics[key] = value.tolist()\n",
    "        return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "acd52967",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSemanticSegmentation, TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "aadc29e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of SegformerForSemanticSegmentation were not initialized from the model checkpoint at nvidia/mit-b0 and are newly initialized: ['decode_head.linear_c.3.proj.bias', 'decode_head.linear_c.1.proj.weight', 'decode_head.batch_norm.num_batches_tracked', 'decode_head.linear_c.2.proj.bias', 'decode_head.linear_c.0.proj.weight', 'decode_head.linear_c.3.proj.weight', 'decode_head.linear_c.1.proj.bias', 'decode_head.batch_norm.bias', 'decode_head.batch_norm.running_var', 'decode_head.classifier.weight', 'decode_head.batch_norm.weight', 'decode_head.linear_c.2.proj.weight', 'decode_head.linear_c.0.proj.bias', 'decode_head.linear_fuse.weight', 'decode_head.classifier.bias', 'decode_head.batch_norm.running_mean']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSemanticSegmentation.from_pretrained(checkpoint, id2label=id2label, label2id=label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "de79f3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "from models import *\n",
    "from strategies import *\n",
    "from custom_datasets import *\n",
    "import numpy as np\n",
    "import tqdm\n",
    "\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "import segmentation_models_pytorch as smp\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "torch.cuda.empty_cache()\n",
    "from PIL import Image\n",
    "import torchvision.transforms as T\n",
    "import torchvision.transforms.functional as TF\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import time\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a38ebcf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_path = \"/root/Master_Thesis/\"\n",
    "dataframes_path = main_path + \"data/dataframes/\"\n",
    "sam_path = main_path + \"sam/sam_vit_h_4b8939.pth\"\n",
    "expirements_path = main_path+\"expirements/\"\n",
    "# train_df = pd.read_csv(dataframes_path+\"train_df.csv\")\n",
    "# test_df = pd.read_csv(dataframes_path+\"test_df.csv\")\n",
    "# train_df = train_df[:4096]\n",
    "# test_df = test_df[:1024]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "67d6da3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/root/Master_Thesis/data/dataframes/brain_df.csv\")\n",
    "df_name = \"brain_df\"\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df, test_df = train_test_split(df, test_size=0.3, random_state=42, shuffle=True)\n",
    "\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "test_df = test_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "504dd28c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1179"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "323fc0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# params = {'n_epoch': 1, \n",
    "#                'train_args':{'batch_size': 1, 'num_workers': 0},\n",
    "#                'test_args':{'batch_size': 1, 'num_workers': 0},\n",
    "#                'optimizer_args':{'lr': 0.00005, 'momentum': 0.5}}\n",
    "\n",
    "params = {'n_epoch': 10,\n",
    "          'train_args':{'batch_size': 8, 'num_workers': 4},\n",
    "          'test_args':{'batch_size': 8, 'num_workers': 4},\n",
    "          'optimizer_args':{'lr': 0.005, 'momentum': 0.9},\n",
    "          'use_sam': False,\n",
    "          'init_set_size': 20, #int(0.1*len(test_df)),\n",
    "          'query_num': 4, #int(0.1*len(test_df)),\n",
    "          'rounds': 10,\n",
    "          'test_set_size': len(test_df),\n",
    "          'df': df_name,\n",
    "          'model': \"SegForm\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fa8bd6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "if params['use_sam']:\n",
    "    sam = SAMOracle(checkpoint_path=sam_path)\n",
    "else:\n",
    "    sam =None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b2391957",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chorus_data(handler, train_df, test_df):\n",
    "    return Data(train_df[\"images\"].to_list(), train_df[\"masks\"].to_list(), test_df[\"images\"].to_list(), test_df[\"masks\"].to_list(), handler, df=train_df, path= main_path+\"/data/processed/\", use_sam=params['use_sam'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3560c909",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_chorus_data(Handler, train_df, test_df)\n",
    "data.initialize_labels(params[\"init_set_size\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cb9b81f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    }
   ],
   "source": [
    "labeled_idxs, labeled_data = data.get_labeled_data()\n",
    "loader = DataLoader(labeled_data, shuffle=True, **params['train_args'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "151447e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch_idx, (x, y, idxs) in enumerate(loader):\n",
    "    out = model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c4cec7a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 1, 64, 64])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "38ad2ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preds = nn.functional.interpolate(out.logits,\n",
    "#                                 size=y.shape[-2:],\n",
    "#                                 mode=\"bilinear\",\n",
    "#                                 align_corners=False,\n",
    "#                             ).argmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7512904c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 256, 256])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "289a8a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preds = preds.type(torch.DoubleTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f622d736",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(preds.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5aafd0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ced01f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.modeling_outputs import SemanticSegmenterOutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c48ce955",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if type(out) is SemanticSegmenterOutput:\n",
    "#     print(\"True\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3225b1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net(model, params, device = torch.device(\"cuda:1\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9121b9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy = MarginSampling(dataset=data, net=net, sam=sam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "86ca7ff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 0\n",
      "20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████| 10/10 [00:08<00:00,  1.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 0 testing metrics: (tensor(0.), tensor(0.))\n",
      "Round 1\n",
      "Querying\n",
      "[1838 1828 1829 1830]\n",
      "Updating without sam\n",
      "Reset and train\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for SegformerForSemanticSegmentation:\n\tMissing key(s) in state_dict: \"segformer.encoder.patch_embeddings.0.proj.weight\", \"segformer.encoder.patch_embeddings.0.proj.bias\", \"segformer.encoder.patch_embeddings.0.layer_norm.weight\", \"segformer.encoder.patch_embeddings.0.layer_norm.bias\", \"segformer.encoder.patch_embeddings.1.proj.weight\", \"segformer.encoder.patch_embeddings.1.proj.bias\", \"segformer.encoder.patch_embeddings.1.layer_norm.weight\", \"segformer.encoder.patch_embeddings.1.layer_norm.bias\", \"segformer.encoder.patch_embeddings.2.proj.weight\", \"segformer.encoder.patch_embeddings.2.proj.bias\", \"segformer.encoder.patch_embeddings.2.layer_norm.weight\", \"segformer.encoder.patch_embeddings.2.layer_norm.bias\", \"segformer.encoder.patch_embeddings.3.proj.weight\", \"segformer.encoder.patch_embeddings.3.proj.bias\", \"segformer.encoder.patch_embeddings.3.layer_norm.weight\", \"segformer.encoder.patch_embeddings.3.layer_norm.bias\", \"segformer.encoder.block.0.0.layer_norm_1.weight\", \"segformer.encoder.block.0.0.layer_norm_1.bias\", \"segformer.encoder.block.0.0.attention.self.query.weight\", \"segformer.encoder.block.0.0.attention.self.query.bias\", \"segformer.encoder.block.0.0.attention.self.key.weight\", \"segformer.encoder.block.0.0.attention.self.key.bias\", \"segformer.encoder.block.0.0.attention.self.value.weight\", \"segformer.encoder.block.0.0.attention.self.value.bias\", \"segformer.encoder.block.0.0.attention.self.sr.weight\", \"segformer.encoder.block.0.0.attention.self.sr.bias\", \"segformer.encoder.block.0.0.attention.self.layer_norm.weight\", \"segformer.encoder.block.0.0.attention.self.layer_norm.bias\", \"segformer.encoder.block.0.0.attention.output.dense.weight\", \"segformer.encoder.block.0.0.attention.output.dense.bias\", \"segformer.encoder.block.0.0.layer_norm_2.weight\", \"segformer.encoder.block.0.0.layer_norm_2.bias\", \"segformer.encoder.block.0.0.mlp.dense1.weight\", \"segformer.encoder.block.0.0.mlp.dense1.bias\", \"segformer.encoder.block.0.0.mlp.dwconv.dwconv.weight\", \"segformer.encoder.block.0.0.mlp.dwconv.dwconv.bias\", \"segformer.encoder.block.0.0.mlp.dense2.weight\", \"segformer.encoder.block.0.0.mlp.dense2.bias\", \"segformer.encoder.block.0.1.layer_norm_1.weight\", \"segformer.encoder.block.0.1.layer_norm_1.bias\", \"segformer.encoder.block.0.1.attention.self.query.weight\", \"segformer.encoder.block.0.1.attention.self.query.bias\", \"segformer.encoder.block.0.1.attention.self.key.weight\", \"segformer.encoder.block.0.1.attention.self.key.bias\", \"segformer.encoder.block.0.1.attention.self.value.weight\", \"segformer.encoder.block.0.1.attention.self.value.bias\", \"segformer.encoder.block.0.1.attention.self.sr.weight\", \"segformer.encoder.block.0.1.attention.self.sr.bias\", \"segformer.encoder.block.0.1.attention.self.layer_norm.weight\", \"segformer.encoder.block.0.1.attention.self.layer_norm.bias\", \"segformer.encoder.block.0.1.attention.output.dense.weight\", \"segformer.encoder.block.0.1.attention.output.dense.bias\", \"segformer.encoder.block.0.1.layer_norm_2.weight\", \"segformer.encoder.block.0.1.layer_norm_2.bias\", \"segformer.encoder.block.0.1.mlp.dense1.weight\", \"segformer.encoder.block.0.1.mlp.dense1.bias\", \"segformer.encoder.block.0.1.mlp.dwconv.dwconv.weight\", \"segformer.encoder.block.0.1.mlp.dwconv.dwconv.bias\", \"segformer.encoder.block.0.1.mlp.dense2.weight\", \"segformer.encoder.block.0.1.mlp.dense2.bias\", \"segformer.encoder.block.1.0.layer_norm_1.weight\", \"segformer.encoder.block.1.0.layer_norm_1.bias\", \"segformer.encoder.block.1.0.attention.self.query.weight\", \"segformer.encoder.block.1.0.attention.self.query.bias\", \"segformer.encoder.block.1.0.attention.self.key.weight\", \"segformer.encoder.block.1.0.attention.self.key.bias\", \"segformer.encoder.block.1.0.attention.self.value.weight\", \"segformer.encoder.block.1.0.attention.self.value.bias\", \"segformer.encoder.block.1.0.attention.self.sr.weight\", \"segformer.encoder.block.1.0.attention.self.sr.bias\", \"segformer.encoder.block.1.0.attention.self.layer_norm.weight\", \"segformer.encoder.block.1.0.attention.self.layer_norm.bias\", \"segformer.encoder.block.1.0.attention.output.dense.weight\", \"segformer.encoder.block.1.0.attention.output.dense.bias\", \"segformer.encoder.block.1.0.layer_norm_2.weight\", \"segformer.encoder.block.1.0.layer_norm_2.bias\", \"segformer.encoder.block.1.0.mlp.dense1.weight\", \"segformer.encoder.block.1.0.mlp.dense1.bias\", \"segformer.encoder.block.1.0.mlp.dwconv.dwconv.weight\", \"segformer.encoder.block.1.0.mlp.dwconv.dwconv.bias\", \"segformer.encoder.block.1.0.mlp.dense2.weight\", \"segformer.encoder.block.1.0.mlp.dense2.bias\", \"segformer.encoder.block.1.1.layer_norm_1.weight\", \"segformer.encoder.block.1.1.layer_norm_1.bias\", \"segformer.encoder.block.1.1.attention.self.query.weight\", \"segformer.encoder.block.1.1.attention.self.query.bias\", \"segformer.encoder.block.1.1.attention.self.key.weight\", \"segformer.encoder.block.1.1.attention.self.key.bias\", \"segformer.encoder.block.1.1.attention.self.value.weight\", \"segformer.encoder.block.1.1.attention.self.value.bias\", \"segformer.encoder.block.1.1.attention.self.sr.weight\", \"segformer.encoder.block.1.1.attention.self.sr.bias\", \"segformer.encoder.block.1.1.attention.self.layer_norm.weight\", \"segformer.encoder.block.1.1.attention.self.layer_norm.bias\", \"segformer.encoder.block.1.1.attention.output.dense.weight\", \"segformer.encoder.block.1.1.attention.output.dense.bias\", \"segformer.encoder.block.1.1.layer_norm_2.weight\", \"segformer.encoder.block.1.1.layer_norm_2.bias\", \"segformer.encoder.block.1.1.mlp.dense1.weight\", \"segformer.encoder.block.1.1.mlp.dense1.bias\", \"segformer.encoder.block.1.1.mlp.dwconv.dwconv.weight\", \"segformer.encoder.block.1.1.mlp.dwconv.dwconv.bias\", \"segformer.encoder.block.1.1.mlp.dense2.weight\", \"segformer.encoder.block.1.1.mlp.dense2.bias\", \"segformer.encoder.block.2.0.layer_norm_1.weight\", \"segformer.encoder.block.2.0.layer_norm_1.bias\", \"segformer.encoder.block.2.0.attention.self.query.weight\", \"segformer.encoder.block.2.0.attention.self.query.bias\", \"segformer.encoder.block.2.0.attention.self.key.weight\", \"segformer.encoder.block.2.0.attention.self.key.bias\", \"segformer.encoder.block.2.0.attention.self.value.weight\", \"segformer.encoder.block.2.0.attention.self.value.bias\", \"segformer.encoder.block.2.0.attention.self.sr.weight\", \"segformer.encoder.block.2.0.attention.self.sr.bias\", \"segformer.encoder.block.2.0.attention.self.layer_norm.weight\", \"segformer.encoder.block.2.0.attention.self.layer_norm.bias\", \"segformer.encoder.block.2.0.attention.output.dense.weight\", \"segformer.encoder.block.2.0.attention.output.dense.bias\", \"segformer.encoder.block.2.0.layer_norm_2.weight\", \"segformer.encoder.block.2.0.layer_norm_2.bias\", \"segformer.encoder.block.2.0.mlp.dense1.weight\", \"segformer.encoder.block.2.0.mlp.dense1.bias\", \"segformer.encoder.block.2.0.mlp.dwconv.dwconv.weight\", \"segformer.encoder.block.2.0.mlp.dwconv.dwconv.bias\", \"segformer.encoder.block.2.0.mlp.dense2.weight\", \"segformer.encoder.block.2.0.mlp.dense2.bias\", \"segformer.encoder.block.2.1.layer_norm_1.weight\", \"segformer.encoder.block.2.1.layer_norm_1.bias\", \"segformer.encoder.block.2.1.attention.self.query.weight\", \"segformer.encoder.block.2.1.attention.self.query.bias\", \"segformer.encoder.block.2.1.attention.self.key.weight\", \"segformer.encoder.block.2.1.attention.self.key.bias\", \"segformer.encoder.block.2.1.attention.self.value.weight\", \"segformer.encoder.block.2.1.attention.self.value.bias\", \"segformer.encoder.block.2.1.attention.self.sr.weight\", \"segformer.encoder.block.2.1.attention.self.sr.bias\", \"segformer.encoder.block.2.1.attention.self.layer_norm.weight\", \"segformer.encoder.block.2.1.attention.self.layer_norm.bias\", \"segformer.encoder.block.2.1.attention.output.dense.weight\", \"segformer.encoder.block.2.1.attention.output.dense.bias\", \"segformer.encoder.block.2.1.layer_norm_2.weight\", \"segformer.encoder.block.2.1.layer_norm_2.bias\", \"segformer.encoder.block.2.1.mlp.dense1.weight\", \"segformer.encoder.block.2.1.mlp.dense1.bias\", \"segformer.encoder.block.2.1.mlp.dwconv.dwconv.weight\", \"segformer.encoder.block.2.1.mlp.dwconv.dwconv.bias\", \"segformer.encoder.block.2.1.mlp.dense2.weight\", \"segformer.encoder.block.2.1.mlp.dense2.bias\", \"segformer.encoder.block.3.0.layer_norm_1.weight\", \"segformer.encoder.block.3.0.layer_norm_1.bias\", \"segformer.encoder.block.3.0.attention.self.query.weight\", \"segformer.encoder.block.3.0.attention.self.query.bias\", \"segformer.encoder.block.3.0.attention.self.key.weight\", \"segformer.encoder.block.3.0.attention.self.key.bias\", \"segformer.encoder.block.3.0.attention.self.value.weight\", \"segformer.encoder.block.3.0.attention.self.value.bias\", \"segformer.encoder.block.3.0.attention.output.dense.weight\", \"segformer.encoder.block.3.0.attention.output.dense.bias\", \"segformer.encoder.block.3.0.layer_norm_2.weight\", \"segformer.encoder.block.3.0.layer_norm_2.bias\", \"segformer.encoder.block.3.0.mlp.dense1.weight\", \"segformer.encoder.block.3.0.mlp.dense1.bias\", \"segformer.encoder.block.3.0.mlp.dwconv.dwconv.weight\", \"segformer.encoder.block.3.0.mlp.dwconv.dwconv.bias\", \"segformer.encoder.block.3.0.mlp.dense2.weight\", \"segformer.encoder.block.3.0.mlp.dense2.bias\", \"segformer.encoder.block.3.1.layer_norm_1.weight\", \"segformer.encoder.block.3.1.layer_norm_1.bias\", \"segformer.encoder.block.3.1.attention.self.query.weight\", \"segformer.encoder.block.3.1.attention.self.query.bias\", \"segformer.encoder.block.3.1.attention.self.key.weight\", \"segformer.encoder.block.3.1.attention.self.key.bias\", \"segformer.encoder.block.3.1.attention.self.value.weight\", \"segformer.encoder.block.3.1.attention.self.value.bias\", \"segformer.encoder.block.3.1.attention.output.dense.weight\", \"segformer.encoder.block.3.1.attention.output.dense.bias\", \"segformer.encoder.block.3.1.layer_norm_2.weight\", \"segformer.encoder.block.3.1.layer_norm_2.bias\", \"segformer.encoder.block.3.1.mlp.dense1.weight\", \"segformer.encoder.block.3.1.mlp.dense1.bias\", \"segformer.encoder.block.3.1.mlp.dwconv.dwconv.weight\", \"segformer.encoder.block.3.1.mlp.dwconv.dwconv.bias\", \"segformer.encoder.block.3.1.mlp.dense2.weight\", \"segformer.encoder.block.3.1.mlp.dense2.bias\", \"segformer.encoder.layer_norm.0.weight\", \"segformer.encoder.layer_norm.0.bias\", \"segformer.encoder.layer_norm.1.weight\", \"segformer.encoder.layer_norm.1.bias\", \"segformer.encoder.layer_norm.2.weight\", \"segformer.encoder.layer_norm.2.bias\", \"segformer.encoder.layer_norm.3.weight\", \"segformer.encoder.layer_norm.3.bias\", \"decode_head.linear_c.0.proj.weight\", \"decode_head.linear_c.0.proj.bias\", \"decode_head.linear_c.1.proj.weight\", \"decode_head.linear_c.1.proj.bias\", \"decode_head.linear_c.2.proj.weight\", \"decode_head.linear_c.2.proj.bias\", \"decode_head.linear_c.3.proj.weight\", \"decode_head.linear_c.3.proj.bias\", \"decode_head.linear_fuse.weight\", \"decode_head.batch_norm.weight\", \"decode_head.batch_norm.bias\", \"decode_head.batch_norm.running_mean\", \"decode_head.batch_norm.running_var\", \"decode_head.classifier.weight\", \"decode_head.classifier.bias\". \n\tUnexpected key(s) in state_dict: \"encoder.conv1.weight\", \"encoder.bn1.weight\", \"encoder.bn1.bias\", \"encoder.bn1.running_mean\", \"encoder.bn1.running_var\", \"encoder.bn1.num_batches_tracked\", \"encoder.layer1.0.conv1.weight\", \"encoder.layer1.0.bn1.weight\", \"encoder.layer1.0.bn1.bias\", \"encoder.layer1.0.bn1.running_mean\", \"encoder.layer1.0.bn1.running_var\", \"encoder.layer1.0.bn1.num_batches_tracked\", \"encoder.layer1.0.conv2.weight\", \"encoder.layer1.0.bn2.weight\", \"encoder.layer1.0.bn2.bias\", \"encoder.layer1.0.bn2.running_mean\", \"encoder.layer1.0.bn2.running_var\", \"encoder.layer1.0.bn2.num_batches_tracked\", \"encoder.layer1.1.conv1.weight\", \"encoder.layer1.1.bn1.weight\", \"encoder.layer1.1.bn1.bias\", \"encoder.layer1.1.bn1.running_mean\", \"encoder.layer1.1.bn1.running_var\", \"encoder.layer1.1.bn1.num_batches_tracked\", \"encoder.layer1.1.conv2.weight\", \"encoder.layer1.1.bn2.weight\", \"encoder.layer1.1.bn2.bias\", \"encoder.layer1.1.bn2.running_mean\", \"encoder.layer1.1.bn2.running_var\", \"encoder.layer1.1.bn2.num_batches_tracked\", \"encoder.layer1.2.conv1.weight\", \"encoder.layer1.2.bn1.weight\", \"encoder.layer1.2.bn1.bias\", \"encoder.layer1.2.bn1.running_mean\", \"encoder.layer1.2.bn1.running_var\", \"encoder.layer1.2.bn1.num_batches_tracked\", \"encoder.layer1.2.conv2.weight\", \"encoder.layer1.2.bn2.weight\", \"encoder.layer1.2.bn2.bias\", \"encoder.layer1.2.bn2.running_mean\", \"encoder.layer1.2.bn2.running_var\", \"encoder.layer1.2.bn2.num_batches_tracked\", \"encoder.layer2.0.conv1.weight\", \"encoder.layer2.0.bn1.weight\", \"encoder.layer2.0.bn1.bias\", \"encoder.layer2.0.bn1.running_mean\", \"encoder.layer2.0.bn1.running_var\", \"encoder.layer2.0.bn1.num_batches_tracked\", \"encoder.layer2.0.conv2.weight\", \"encoder.layer2.0.bn2.weight\", \"encoder.layer2.0.bn2.bias\", \"encoder.layer2.0.bn2.running_mean\", \"encoder.layer2.0.bn2.running_var\", \"encoder.layer2.0.bn2.num_batches_tracked\", \"encoder.layer2.0.downsample.0.weight\", \"encoder.layer2.0.downsample.1.weight\", \"encoder.layer2.0.downsample.1.bias\", \"encoder.layer2.0.downsample.1.running_mean\", \"encoder.layer2.0.downsample.1.running_var\", \"encoder.layer2.0.downsample.1.num_batches_tracked\", \"encoder.layer2.1.conv1.weight\", \"encoder.layer2.1.bn1.weight\", \"encoder.layer2.1.bn1.bias\", \"encoder.layer2.1.bn1.running_mean\", \"encoder.layer2.1.bn1.running_var\", \"encoder.layer2.1.bn1.num_batches_tracked\", \"encoder.layer2.1.conv2.weight\", \"encoder.layer2.1.bn2.weight\", \"encoder.layer2.1.bn2.bias\", \"encoder.layer2.1.bn2.running_mean\", \"encoder.layer2.1.bn2.running_var\", \"encoder.layer2.1.bn2.num_batches_tracked\", \"encoder.layer2.2.conv1.weight\", \"encoder.layer2.2.bn1.weight\", \"encoder.layer2.2.bn1.bias\", \"encoder.layer2.2.bn1.running_mean\", \"encoder.layer2.2.bn1.running_var\", \"encoder.layer2.2.bn1.num_batches_tracked\", \"encoder.layer2.2.conv2.weight\", \"encoder.layer2.2.bn2.weight\", \"encoder.layer2.2.bn2.bias\", \"encoder.layer2.2.bn2.running_mean\", \"encoder.layer2.2.bn2.running_var\", \"encoder.layer2.2.bn2.num_batches_tracked\", \"encoder.layer2.3.conv1.weight\", \"encoder.layer2.3.bn1.weight\", \"encoder.layer2.3.bn1.bias\", \"encoder.layer2.3.bn1.running_mean\", \"encoder.layer2.3.bn1.running_var\", \"encoder.layer2.3.bn1.num_batches_tracked\", \"encoder.layer2.3.conv2.weight\", \"encoder.layer2.3.bn2.weight\", \"encoder.layer2.3.bn2.bias\", \"encoder.layer2.3.bn2.running_mean\", \"encoder.layer2.3.bn2.running_var\", \"encoder.layer2.3.bn2.num_batches_tracked\", \"encoder.layer3.0.conv1.weight\", \"encoder.layer3.0.bn1.weight\", \"encoder.layer3.0.bn1.bias\", \"encoder.layer3.0.bn1.running_mean\", \"encoder.layer3.0.bn1.running_var\", \"encoder.layer3.0.bn1.num_batches_tracked\", \"encoder.layer3.0.conv2.weight\", \"encoder.layer3.0.bn2.weight\", \"encoder.layer3.0.bn2.bias\", \"encoder.layer3.0.bn2.running_mean\", \"encoder.layer3.0.bn2.running_var\", \"encoder.layer3.0.bn2.num_batches_tracked\", \"encoder.layer3.0.downsample.0.weight\", \"encoder.layer3.0.downsample.1.weight\", \"encoder.layer3.0.downsample.1.bias\", \"encoder.layer3.0.downsample.1.running_mean\", \"encoder.layer3.0.downsample.1.running_var\", \"encoder.layer3.0.downsample.1.num_batches_tracked\", \"encoder.layer3.1.conv1.weight\", \"encoder.layer3.1.bn1.weight\", \"encoder.layer3.1.bn1.bias\", \"encoder.layer3.1.bn1.running_mean\", \"encoder.layer3.1.bn1.running_var\", \"encoder.layer3.1.bn1.num_batches_tracked\", \"encoder.layer3.1.conv2.weight\", \"encoder.layer3.1.bn2.weight\", \"encoder.layer3.1.bn2.bias\", \"encoder.layer3.1.bn2.running_mean\", \"encoder.layer3.1.bn2.running_var\", \"encoder.layer3.1.bn2.num_batches_tracked\", \"encoder.layer3.2.conv1.weight\", \"encoder.layer3.2.bn1.weight\", \"encoder.layer3.2.bn1.bias\", \"encoder.layer3.2.bn1.running_mean\", \"encoder.layer3.2.bn1.running_var\", \"encoder.layer3.2.bn1.num_batches_tracked\", \"encoder.layer3.2.conv2.weight\", \"encoder.layer3.2.bn2.weight\", \"encoder.layer3.2.bn2.bias\", \"encoder.layer3.2.bn2.running_mean\", \"encoder.layer3.2.bn2.running_var\", \"encoder.layer3.2.bn2.num_batches_tracked\", \"encoder.layer3.3.conv1.weight\", \"encoder.layer3.3.bn1.weight\", \"encoder.layer3.3.bn1.bias\", \"encoder.layer3.3.bn1.running_mean\", \"encoder.layer3.3.bn1.running_var\", \"encoder.layer3.3.bn1.num_batches_tracked\", \"encoder.layer3.3.conv2.weight\", \"encoder.layer3.3.bn2.weight\", \"encoder.layer3.3.bn2.bias\", \"encoder.layer3.3.bn2.running_mean\", \"encoder.layer3.3.bn2.running_var\", \"encoder.layer3.3.bn2.num_batches_tracked\", \"encoder.layer3.4.conv1.weight\", \"encoder.layer3.4.bn1.weight\", \"encoder.layer3.4.bn1.bias\", \"encoder.layer3.4.bn1.running_mean\", \"encoder.layer3.4.bn1.running_var\", \"encoder.layer3.4.bn1.num_batches_tracked\", \"encoder.layer3.4.conv2.weight\", \"encoder.layer3.4.bn2.weight\", \"encoder.layer3.4.bn2.bias\", \"encoder.layer3.4.bn2.running_mean\", \"encoder.layer3.4.bn2.running_var\", \"encoder.layer3.4.bn2.num_batches_tracked\", \"encoder.layer3.5.conv1.weight\", \"encoder.layer3.5.bn1.weight\", \"encoder.layer3.5.bn1.bias\", \"encoder.layer3.5.bn1.running_mean\", \"encoder.layer3.5.bn1.running_var\", \"encoder.layer3.5.bn1.num_batches_tracked\", \"encoder.layer3.5.conv2.weight\", \"encoder.layer3.5.bn2.weight\", \"encoder.layer3.5.bn2.bias\", \"encoder.layer3.5.bn2.running_mean\", \"encoder.layer3.5.bn2.running_var\", \"encoder.layer3.5.bn2.num_batches_tracked\", \"encoder.layer4.0.conv1.weight\", \"encoder.layer4.0.bn1.weight\", \"encoder.layer4.0.bn1.bias\", \"encoder.layer4.0.bn1.running_mean\", \"encoder.layer4.0.bn1.running_var\", \"encoder.layer4.0.bn1.num_batches_tracked\", \"encoder.layer4.0.conv2.weight\", \"encoder.layer4.0.bn2.weight\", \"encoder.layer4.0.bn2.bias\", \"encoder.layer4.0.bn2.running_mean\", \"encoder.layer4.0.bn2.running_var\", \"encoder.layer4.0.bn2.num_batches_tracked\", \"encoder.layer4.0.downsample.0.weight\", \"encoder.layer4.0.downsample.1.weight\", \"encoder.layer4.0.downsample.1.bias\", \"encoder.layer4.0.downsample.1.running_mean\", \"encoder.layer4.0.downsample.1.running_var\", \"encoder.layer4.0.downsample.1.num_batches_tracked\", \"encoder.layer4.1.conv1.weight\", \"encoder.layer4.1.bn1.weight\", \"encoder.layer4.1.bn1.bias\", \"encoder.layer4.1.bn1.running_mean\", \"encoder.layer4.1.bn1.running_var\", \"encoder.layer4.1.bn1.num_batches_tracked\", \"encoder.layer4.1.conv2.weight\", \"encoder.layer4.1.bn2.weight\", \"encoder.layer4.1.bn2.bias\", \"encoder.layer4.1.bn2.running_mean\", \"encoder.layer4.1.bn2.running_var\", \"encoder.layer4.1.bn2.num_batches_tracked\", \"encoder.layer4.2.conv1.weight\", \"encoder.layer4.2.bn1.weight\", \"encoder.layer4.2.bn1.bias\", \"encoder.layer4.2.bn1.running_mean\", \"encoder.layer4.2.bn1.running_var\", \"encoder.layer4.2.bn1.num_batches_tracked\", \"encoder.layer4.2.conv2.weight\", \"encoder.layer4.2.bn2.weight\", \"encoder.layer4.2.bn2.bias\", \"encoder.layer4.2.bn2.running_mean\", \"encoder.layer4.2.bn2.running_var\", \"encoder.layer4.2.bn2.num_batches_tracked\", \"decoder.p5.weight\", \"decoder.p5.bias\", \"decoder.p4.skip_conv.weight\", \"decoder.p4.skip_conv.bias\", \"decoder.p3.skip_conv.weight\", \"decoder.p3.skip_conv.bias\", \"decoder.p2.skip_conv.weight\", \"decoder.p2.skip_conv.bias\", \"decoder.seg_blocks.0.block.0.block.0.weight\", \"decoder.seg_blocks.0.block.0.block.1.weight\", \"decoder.seg_blocks.0.block.0.block.1.bias\", \"decoder.seg_blocks.0.block.1.block.0.weight\", \"decoder.seg_blocks.0.block.1.block.1.weight\", \"decoder.seg_blocks.0.block.1.block.1.bias\", \"decoder.seg_blocks.0.block.2.block.0.weight\", \"decoder.seg_blocks.0.block.2.block.1.weight\", \"decoder.seg_blocks.0.block.2.block.1.bias\", \"decoder.seg_blocks.1.block.0.block.0.weight\", \"decoder.seg_blocks.1.block.0.block.1.weight\", \"decoder.seg_blocks.1.block.0.block.1.bias\", \"decoder.seg_blocks.1.block.1.block.0.weight\", \"decoder.seg_blocks.1.block.1.block.1.weight\", \"decoder.seg_blocks.1.block.1.block.1.bias\", \"decoder.seg_blocks.2.block.0.block.0.weight\", \"decoder.seg_blocks.2.block.0.block.1.weight\", \"decoder.seg_blocks.2.block.0.block.1.bias\", \"decoder.seg_blocks.3.block.0.block.0.weight\", \"decoder.seg_blocks.3.block.0.block.1.weight\", \"decoder.seg_blocks.3.block.0.block.1.bias\", \"segmentation_head.0.weight\", \"segmentation_head.0.bias\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[46], line 27\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReset and train\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     26\u001b[0m init_state \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minit_state.pt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 27\u001b[0m \u001b[43mstrategy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43minit_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m strategy\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# calculate accuracy\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/al_env/lib/python3.9/site-packages/torch/nn/modules/module.py:2041\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   2036\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2037\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2038\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(k) \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[1;32m   2040\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2041\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2042\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   2043\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for SegformerForSemanticSegmentation:\n\tMissing key(s) in state_dict: \"segformer.encoder.patch_embeddings.0.proj.weight\", \"segformer.encoder.patch_embeddings.0.proj.bias\", \"segformer.encoder.patch_embeddings.0.layer_norm.weight\", \"segformer.encoder.patch_embeddings.0.layer_norm.bias\", \"segformer.encoder.patch_embeddings.1.proj.weight\", \"segformer.encoder.patch_embeddings.1.proj.bias\", \"segformer.encoder.patch_embeddings.1.layer_norm.weight\", \"segformer.encoder.patch_embeddings.1.layer_norm.bias\", \"segformer.encoder.patch_embeddings.2.proj.weight\", \"segformer.encoder.patch_embeddings.2.proj.bias\", \"segformer.encoder.patch_embeddings.2.layer_norm.weight\", \"segformer.encoder.patch_embeddings.2.layer_norm.bias\", \"segformer.encoder.patch_embeddings.3.proj.weight\", \"segformer.encoder.patch_embeddings.3.proj.bias\", \"segformer.encoder.patch_embeddings.3.layer_norm.weight\", \"segformer.encoder.patch_embeddings.3.layer_norm.bias\", \"segformer.encoder.block.0.0.layer_norm_1.weight\", \"segformer.encoder.block.0.0.layer_norm_1.bias\", \"segformer.encoder.block.0.0.attention.self.query.weight\", \"segformer.encoder.block.0.0.attention.self.query.bias\", \"segformer.encoder.block.0.0.attention.self.key.weight\", \"segformer.encoder.block.0.0.attention.self.key.bias\", \"segformer.encoder.block.0.0.attention.self.value.weight\", \"segformer.encoder.block.0.0.attention.self.value.bias\", \"segformer.encoder.block.0.0.attention.self.sr.weight\", \"segformer.encoder.block.0.0.attention.self.sr.bias\", \"segformer.encoder.block.0.0.attention.self.layer_norm.weight\", \"segformer.encoder.block.0.0.attention.self.layer_norm.bias\", \"segformer.encoder.block.0.0.attention.output.dense.weight\", \"segformer.encoder.block.0.0.attention.output.dense.bias\", \"segformer.encoder.block.0.0.layer_norm_2.weight\", \"segformer.encoder.block.0.0.layer_norm_2.bias\", \"segformer.encoder.block.0.0.mlp.dense1.weight\", \"segformer.encoder.block.0.0.mlp.dense1.bias\", \"segformer.encoder.block.0.0.mlp.dwconv.dwconv.weight\", \"segformer.encoder.block.0.0.mlp.dwconv.dwconv.bias\", \"segformer.encoder.block.0.0.mlp.dense2.weight\", \"segformer.encoder.block.0.0.mlp.dense2.bias\", \"segformer.encoder.block.0.1.layer_norm_1.weight\", \"segformer.encoder.block.0.1.layer_norm_1.bias\", \"segformer.encoder.block.0.1.attention.self.query.weight\", \"segformer.encoder.block.0.1.attention.self.query.bias\", \"segformer.encoder.block.0.1.attention.self.key.weight\", \"segformer.encoder.block.0.1.attention.self.key.bias\", \"segformer.encoder.block.0.1.attention.self.value.weight\", \"segformer.encoder.block.0.1.attention.self.value.bias\", \"segformer.encoder.block.0.1.attention.self.sr.weight\", \"segformer.encoder.block.0.1.attention.self.sr.bias\", \"segformer.encoder.block.0.1.attention.self.layer_norm.weight\", \"segformer.encoder.block.0.1.attention.self.layer_norm.bias\", \"segformer.encoder.block.0.1.attention.output.dense.weight\", \"segformer.encoder.block.0.1.attention.output.dense.bias\", \"segformer.encoder.block.0.1.layer_norm_2.weight\", \"segformer.encoder.block.0.1.layer_norm_2.bias\", \"segformer.encoder.block.0.1.mlp.dense1.weight\", \"segformer.encoder.block.0.1.mlp.dense1.bias\", \"segformer.encoder.block.0.1.mlp.dwconv.dwconv.weight\", \"segformer.encoder.block.0.1.mlp.dwconv.dwconv.bias\", \"segformer.encoder.block.0.1.mlp.dense2.weight\", \"segformer.encoder.block.0.1.mlp.dense2.bias\", \"segformer.encoder.block.1.0.layer_norm_1.weight\", \"segformer.encoder.block.1.0.layer_norm_1.bias\", \"segformer.encoder.block.1.0.attention.self.query.weight\", \"segformer.encoder.block.1.0.attention.self.query.bias\", \"segformer.encoder.block.1.0.attention.self.key.weight\", \"segformer.encoder.block.1.0.attention.self.key.bias\", \"segformer.encoder.block.1.0.attention.self.value.weight\", \"segformer.encoder.block.1.0.attention.self.value.bias\", \"segformer.encoder.block.1.0.attention.self.sr.weight\", \"segformer.encoder.block.1.0.attention.self.sr.bias\", \"segformer.encoder.block.1.0.attention.self.layer_norm.weight\", \"segformer.encoder.block.1.0.attention.self.layer_norm.bias\", \"segformer.encoder.block.1.0.attention.output.dense.weight\", \"segformer.encoder.block.1.0.attention.output.dense.bias\", \"segformer.encoder.block.1.0.layer_norm_2.weight\", \"segformer.encoder.block.1.0.layer_norm_2.bias\", \"segformer.encoder.block.1.0.mlp.dense1.weight\", \"segformer.encoder.block.1.0.mlp.dense1.bias\", \"segformer.encoder.block.1.0.mlp.dwconv.dwconv.weight\", \"segformer.encoder.block.1.0.mlp.dwconv.dwconv.bias\", \"segformer.encoder.block.1.0.mlp.dense2.weight\", \"segformer.encoder.block.1.0.mlp.dense2.bias\", \"segformer.encoder.block.1.1.layer_norm_1.weight\", \"segformer.encoder.block.1.1.layer_norm_1.bias\", \"segformer.encoder.block.1.1.attention.self.query.weight\", \"segformer.encoder.block.1.1.attention.self.query.bias\", \"segformer.encoder.block.1.1.attention.self.key.weight\", \"segformer.encoder.block.1.1.attention.self.key.bias\", \"segformer.encoder.block.1.1.attention.self.value.weight\", \"segformer.encoder.block.1.1.attention.self.value.bias\", \"segformer.encoder.block.1.1.attention.self.sr.weight\", \"segformer.encoder.block.1.1.attention.self.sr.bias\", \"segformer.encoder.block.1.1.attention.self.layer_norm.weight\", \"segformer.encoder.block.1.1.attention.self.layer_norm.bias\", \"segformer.encoder.block.1.1.attention.output.dense.weight\", \"segformer.encoder.block.1.1.attention.output.dense.bias\", \"segformer.encoder.block.1.1.layer_norm_2.weight\", \"segformer.encoder.block.1.1.layer_norm_2.bias\", \"segformer.encoder.block.1.1.mlp.dense1.weight\", \"segformer.encoder.block.1.1.mlp.dense1.bias\", \"segformer.encoder.block.1.1.mlp.dwconv.dwconv.weight\", \"segformer.encoder.block.1.1.mlp.dwconv.dwconv.bias\", \"segformer.encoder.block.1.1.mlp.dense2.weight\", \"segformer.encoder.block.1.1.mlp.dense2.bias\", \"segformer.encoder.block.2.0.layer_norm_1.weight\", \"segformer.encoder.block.2.0.layer_norm_1.bias\", \"segformer.encoder.block.2.0.attention.self.query.weight\", \"segformer.encoder.block.2.0.attention.self.query.bias\", \"segformer.encoder.block.2.0.attention.self.key.weight\", \"segformer.encoder.block.2.0.attention.self.key.bias\", \"segformer.encoder.block.2.0.attention.self.value.weight\", \"segformer.encoder.block.2.0.attention.self.value.bias\", \"segformer.encoder.block.2.0.attention.self.sr.weight\", \"segformer.encoder.block.2.0.attention.self.sr.bias\", \"segformer.encoder.block.2.0.attention.self.layer_norm.weight\", \"segformer.encoder.block.2.0.attention.self.layer_norm.bias\", \"segformer.encoder.block.2.0.attention.output.dense.weight\", \"segformer.encoder.block.2.0.attention.output.dense.bias\", \"segformer.encoder.block.2.0.layer_norm_2.weight\", \"segformer.encoder.block.2.0.layer_norm_2.bias\", \"segformer.encoder.block.2.0.mlp.dense1.weight\", \"segformer.encoder.block.2.0.mlp.dense1.bias\", \"segformer.encoder.block.2.0.mlp.dwconv.dwconv.weight\", \"segformer.encoder.block.2.0.mlp.dwconv.dwconv.bias\", \"segformer.encoder.block.2.0.mlp.dense2.weight\", \"segformer.encoder.block.2.0.mlp.dense2.bias\", \"segformer.encoder.block.2.1.layer_norm_1.weight\", \"segformer.encoder.block.2.1.layer_norm_1.bias\", \"segformer.encoder.block.2.1.attention.self.query.weight\", \"segformer.encoder.block.2.1.attention.self.query.bias\", \"segformer.encoder.block.2.1.attention.self.key.weight\", \"segformer.encoder.block.2.1.attention.self.key.bias\", \"segformer.encoder.block.2.1.attention.self.value.weight\", \"segformer.encoder.block.2.1.attention.self.value.bias\", \"segformer.encoder.block.2.1.attention.self.sr.weight\", \"segformer.encoder.block.2.1.attention.self.sr.bias\", \"segformer.encoder.block.2.1.attention.self.layer_norm.weight\", \"segformer.encoder.block.2.1.attention.self.layer_norm.bias\", \"segformer.encoder.block.2.1.attention.output.dense.weight\", \"segformer.encoder.block.2.1.attention.output.dense.bias\", \"segformer.encoder.block.2.1.layer_norm_2.weight\", \"segformer.encoder.block.2.1.layer_norm_2.bias\", \"segformer.encoder.block.2.1.mlp.dense1.weight\", \"segformer.encoder.block.2.1.mlp.dense1.bias\", \"segformer.encoder.block.2.1.mlp.dwconv.dwconv.weight\", \"segformer.encoder.block.2.1.mlp.dwconv.dwconv.bias\", \"segformer.encoder.block.2.1.mlp.dense2.weight\", \"segformer.encoder.block.2.1.mlp.dense2.bias\", \"segformer.encoder.block.3.0.layer_norm_1.weight\", \"segformer.encoder.block.3.0.layer_norm_1.bias\", \"segformer.encoder.block.3.0.attention.self.query.weight\", \"segformer.encoder.block.3.0.attention.self.query.bias\", \"segformer.encoder.block.3.0.attention.self.key.weight\", \"segformer.encoder.block.3.0.attention.self.key.bias\", \"segformer.encoder.block.3.0.attention.self.value.weight\", \"segformer.encoder.block.3.0.attention.self.value.bias\", \"segformer.encoder.block.3.0.attention.output.dense.weight\", \"segformer.encoder.block.3.0.attention.output.dense.bias\", \"segformer.encoder.block.3.0.layer_norm_2.weight\", \"segformer.encoder.block.3.0.layer_norm_2.bias\", \"segformer.encoder.block.3.0.mlp.dense1.weight\", \"segformer.encoder.block.3.0.mlp.dense1.bias\", \"segformer.encoder.block.3.0.mlp.dwconv.dwconv.weight\", \"segformer.encoder.block.3.0.mlp.dwconv.dwconv.bias\", \"segformer.encoder.block.3.0.mlp.dense2.weight\", \"segformer.encoder.block.3.0.mlp.dense2.bias\", \"segformer.encoder.block.3.1.layer_norm_1.weight\", \"segformer.encoder.block.3.1.layer_norm_1.bias\", \"segformer.encoder.block.3.1.attention.self.query.weight\", \"segformer.encoder.block.3.1.attention.self.query.bias\", \"segformer.encoder.block.3.1.attention.self.key.weight\", \"segformer.encoder.block.3.1.attention.self.key.bias\", \"segformer.encoder.block.3.1.attention.self.value.weight\", \"segformer.encoder.block.3.1.attention.self.value.bias\", \"segformer.encoder.block.3.1.attention.output.dense.weight\", \"segformer.encoder.block.3.1.attention.output.dense.bias\", \"segformer.encoder.block.3.1.layer_norm_2.weight\", \"segformer.encoder.block.3.1.layer_norm_2.bias\", \"segformer.encoder.block.3.1.mlp.dense1.weight\", \"segformer.encoder.block.3.1.mlp.dense1.bias\", \"segformer.encoder.block.3.1.mlp.dwconv.dwconv.weight\", \"segformer.encoder.block.3.1.mlp.dwconv.dwconv.bias\", \"segformer.encoder.block.3.1.mlp.dense2.weight\", \"segformer.encoder.block.3.1.mlp.dense2.bias\", \"segformer.encoder.layer_norm.0.weight\", \"segformer.encoder.layer_norm.0.bias\", \"segformer.encoder.layer_norm.1.weight\", \"segformer.encoder.layer_norm.1.bias\", \"segformer.encoder.layer_norm.2.weight\", \"segformer.encoder.layer_norm.2.bias\", \"segformer.encoder.layer_norm.3.weight\", \"segformer.encoder.layer_norm.3.bias\", \"decode_head.linear_c.0.proj.weight\", \"decode_head.linear_c.0.proj.bias\", \"decode_head.linear_c.1.proj.weight\", \"decode_head.linear_c.1.proj.bias\", \"decode_head.linear_c.2.proj.weight\", \"decode_head.linear_c.2.proj.bias\", \"decode_head.linear_c.3.proj.weight\", \"decode_head.linear_c.3.proj.bias\", \"decode_head.linear_fuse.weight\", \"decode_head.batch_norm.weight\", \"decode_head.batch_norm.bias\", \"decode_head.batch_norm.running_mean\", \"decode_head.batch_norm.running_var\", \"decode_head.classifier.weight\", \"decode_head.classifier.bias\". \n\tUnexpected key(s) in state_dict: \"encoder.conv1.weight\", \"encoder.bn1.weight\", \"encoder.bn1.bias\", \"encoder.bn1.running_mean\", \"encoder.bn1.running_var\", \"encoder.bn1.num_batches_tracked\", \"encoder.layer1.0.conv1.weight\", \"encoder.layer1.0.bn1.weight\", \"encoder.layer1.0.bn1.bias\", \"encoder.layer1.0.bn1.running_mean\", \"encoder.layer1.0.bn1.running_var\", \"encoder.layer1.0.bn1.num_batches_tracked\", \"encoder.layer1.0.conv2.weight\", \"encoder.layer1.0.bn2.weight\", \"encoder.layer1.0.bn2.bias\", \"encoder.layer1.0.bn2.running_mean\", \"encoder.layer1.0.bn2.running_var\", \"encoder.layer1.0.bn2.num_batches_tracked\", \"encoder.layer1.1.conv1.weight\", \"encoder.layer1.1.bn1.weight\", \"encoder.layer1.1.bn1.bias\", \"encoder.layer1.1.bn1.running_mean\", \"encoder.layer1.1.bn1.running_var\", \"encoder.layer1.1.bn1.num_batches_tracked\", \"encoder.layer1.1.conv2.weight\", \"encoder.layer1.1.bn2.weight\", \"encoder.layer1.1.bn2.bias\", \"encoder.layer1.1.bn2.running_mean\", \"encoder.layer1.1.bn2.running_var\", \"encoder.layer1.1.bn2.num_batches_tracked\", \"encoder.layer1.2.conv1.weight\", \"encoder.layer1.2.bn1.weight\", \"encoder.layer1.2.bn1.bias\", \"encoder.layer1.2.bn1.running_mean\", \"encoder.layer1.2.bn1.running_var\", \"encoder.layer1.2.bn1.num_batches_tracked\", \"encoder.layer1.2.conv2.weight\", \"encoder.layer1.2.bn2.weight\", \"encoder.layer1.2.bn2.bias\", \"encoder.layer1.2.bn2.running_mean\", \"encoder.layer1.2.bn2.running_var\", \"encoder.layer1.2.bn2.num_batches_tracked\", \"encoder.layer2.0.conv1.weight\", \"encoder.layer2.0.bn1.weight\", \"encoder.layer2.0.bn1.bias\", \"encoder.layer2.0.bn1.running_mean\", \"encoder.layer2.0.bn1.running_var\", \"encoder.layer2.0.bn1.num_batches_tracked\", \"encoder.layer2.0.conv2.weight\", \"encoder.layer2.0.bn2.weight\", \"encoder.layer2.0.bn2.bias\", \"encoder.layer2.0.bn2.running_mean\", \"encoder.layer2.0.bn2.running_var\", \"encoder.layer2.0.bn2.num_batches_tracked\", \"encoder.layer2.0.downsample.0.weight\", \"encoder.layer2.0.downsample.1.weight\", \"encoder.layer2.0.downsample.1.bias\", \"encoder.layer2.0.downsample.1.running_mean\", \"encoder.layer2.0.downsample.1.running_var\", \"encoder.layer2.0.downsample.1.num_batches_tracked\", \"encoder.layer2.1.conv1.weight\", \"encoder.layer2.1.bn1.weight\", \"encoder.layer2.1.bn1.bias\", \"encoder.layer2.1.bn1.running_mean\", \"encoder.layer2.1.bn1.running_var\", \"encoder.layer2.1.bn1.num_batches_tracked\", \"encoder.layer2.1.conv2.weight\", \"encoder.layer2.1.bn2.weight\", \"encoder.layer2.1.bn2.bias\", \"encoder.layer2.1.bn2.running_mean\", \"encoder.layer2.1.bn2.running_var\", \"encoder.layer2.1.bn2.num_batches_tracked\", \"encoder.layer2.2.conv1.weight\", \"encoder.layer2.2.bn1.weight\", \"encoder.layer2.2.bn1.bias\", \"encoder.layer2.2.bn1.running_mean\", \"encoder.layer2.2.bn1.running_var\", \"encoder.layer2.2.bn1.num_batches_tracked\", \"encoder.layer2.2.conv2.weight\", \"encoder.layer2.2.bn2.weight\", \"encoder.layer2.2.bn2.bias\", \"encoder.layer2.2.bn2.running_mean\", \"encoder.layer2.2.bn2.running_var\", \"encoder.layer2.2.bn2.num_batches_tracked\", \"encoder.layer2.3.conv1.weight\", \"encoder.layer2.3.bn1.weight\", \"encoder.layer2.3.bn1.bias\", \"encoder.layer2.3.bn1.running_mean\", \"encoder.layer2.3.bn1.running_var\", \"encoder.layer2.3.bn1.num_batches_tracked\", \"encoder.layer2.3.conv2.weight\", \"encoder.layer2.3.bn2.weight\", \"encoder.layer2.3.bn2.bias\", \"encoder.layer2.3.bn2.running_mean\", \"encoder.layer2.3.bn2.running_var\", \"encoder.layer2.3.bn2.num_batches_tracked\", \"encoder.layer3.0.conv1.weight\", \"encoder.layer3.0.bn1.weight\", \"encoder.layer3.0.bn1.bias\", \"encoder.layer3.0.bn1.running_mean\", \"encoder.layer3.0.bn1.running_var\", \"encoder.layer3.0.bn1.num_batches_tracked\", \"encoder.layer3.0.conv2.weight\", \"encoder.layer3.0.bn2.weight\", \"encoder.layer3.0.bn2.bias\", \"encoder.layer3.0.bn2.running_mean\", \"encoder.layer3.0.bn2.running_var\", \"encoder.layer3.0.bn2.num_batches_tracked\", \"encoder.layer3.0.downsample.0.weight\", \"encoder.layer3.0.downsample.1.weight\", \"encoder.layer3.0.downsample.1.bias\", \"encoder.layer3.0.downsample.1.running_mean\", \"encoder.layer3.0.downsample.1.running_var\", \"encoder.layer3.0.downsample.1.num_batches_tracked\", \"encoder.layer3.1.conv1.weight\", \"encoder.layer3.1.bn1.weight\", \"encoder.layer3.1.bn1.bias\", \"encoder.layer3.1.bn1.running_mean\", \"encoder.layer3.1.bn1.running_var\", \"encoder.layer3.1.bn1.num_batches_tracked\", \"encoder.layer3.1.conv2.weight\", \"encoder.layer3.1.bn2.weight\", \"encoder.layer3.1.bn2.bias\", \"encoder.layer3.1.bn2.running_mean\", \"encoder.layer3.1.bn2.running_var\", \"encoder.layer3.1.bn2.num_batches_tracked\", \"encoder.layer3.2.conv1.weight\", \"encoder.layer3.2.bn1.weight\", \"encoder.layer3.2.bn1.bias\", \"encoder.layer3.2.bn1.running_mean\", \"encoder.layer3.2.bn1.running_var\", \"encoder.layer3.2.bn1.num_batches_tracked\", \"encoder.layer3.2.conv2.weight\", \"encoder.layer3.2.bn2.weight\", \"encoder.layer3.2.bn2.bias\", \"encoder.layer3.2.bn2.running_mean\", \"encoder.layer3.2.bn2.running_var\", \"encoder.layer3.2.bn2.num_batches_tracked\", \"encoder.layer3.3.conv1.weight\", \"encoder.layer3.3.bn1.weight\", \"encoder.layer3.3.bn1.bias\", \"encoder.layer3.3.bn1.running_mean\", \"encoder.layer3.3.bn1.running_var\", \"encoder.layer3.3.bn1.num_batches_tracked\", \"encoder.layer3.3.conv2.weight\", \"encoder.layer3.3.bn2.weight\", \"encoder.layer3.3.bn2.bias\", \"encoder.layer3.3.bn2.running_mean\", \"encoder.layer3.3.bn2.running_var\", \"encoder.layer3.3.bn2.num_batches_tracked\", \"encoder.layer3.4.conv1.weight\", \"encoder.layer3.4.bn1.weight\", \"encoder.layer3.4.bn1.bias\", \"encoder.layer3.4.bn1.running_mean\", \"encoder.layer3.4.bn1.running_var\", \"encoder.layer3.4.bn1.num_batches_tracked\", \"encoder.layer3.4.conv2.weight\", \"encoder.layer3.4.bn2.weight\", \"encoder.layer3.4.bn2.bias\", \"encoder.layer3.4.bn2.running_mean\", \"encoder.layer3.4.bn2.running_var\", \"encoder.layer3.4.bn2.num_batches_tracked\", \"encoder.layer3.5.conv1.weight\", \"encoder.layer3.5.bn1.weight\", \"encoder.layer3.5.bn1.bias\", \"encoder.layer3.5.bn1.running_mean\", \"encoder.layer3.5.bn1.running_var\", \"encoder.layer3.5.bn1.num_batches_tracked\", \"encoder.layer3.5.conv2.weight\", \"encoder.layer3.5.bn2.weight\", \"encoder.layer3.5.bn2.bias\", \"encoder.layer3.5.bn2.running_mean\", \"encoder.layer3.5.bn2.running_var\", \"encoder.layer3.5.bn2.num_batches_tracked\", \"encoder.layer4.0.conv1.weight\", \"encoder.layer4.0.bn1.weight\", \"encoder.layer4.0.bn1.bias\", \"encoder.layer4.0.bn1.running_mean\", \"encoder.layer4.0.bn1.running_var\", \"encoder.layer4.0.bn1.num_batches_tracked\", \"encoder.layer4.0.conv2.weight\", \"encoder.layer4.0.bn2.weight\", \"encoder.layer4.0.bn2.bias\", \"encoder.layer4.0.bn2.running_mean\", \"encoder.layer4.0.bn2.running_var\", \"encoder.layer4.0.bn2.num_batches_tracked\", \"encoder.layer4.0.downsample.0.weight\", \"encoder.layer4.0.downsample.1.weight\", \"encoder.layer4.0.downsample.1.bias\", \"encoder.layer4.0.downsample.1.running_mean\", \"encoder.layer4.0.downsample.1.running_var\", \"encoder.layer4.0.downsample.1.num_batches_tracked\", \"encoder.layer4.1.conv1.weight\", \"encoder.layer4.1.bn1.weight\", \"encoder.layer4.1.bn1.bias\", \"encoder.layer4.1.bn1.running_mean\", \"encoder.layer4.1.bn1.running_var\", \"encoder.layer4.1.bn1.num_batches_tracked\", \"encoder.layer4.1.conv2.weight\", \"encoder.layer4.1.bn2.weight\", \"encoder.layer4.1.bn2.bias\", \"encoder.layer4.1.bn2.running_mean\", \"encoder.layer4.1.bn2.running_var\", \"encoder.layer4.1.bn2.num_batches_tracked\", \"encoder.layer4.2.conv1.weight\", \"encoder.layer4.2.bn1.weight\", \"encoder.layer4.2.bn1.bias\", \"encoder.layer4.2.bn1.running_mean\", \"encoder.layer4.2.bn1.running_var\", \"encoder.layer4.2.bn1.num_batches_tracked\", \"encoder.layer4.2.conv2.weight\", \"encoder.layer4.2.bn2.weight\", \"encoder.layer4.2.bn2.bias\", \"encoder.layer4.2.bn2.running_mean\", \"encoder.layer4.2.bn2.running_var\", \"encoder.layer4.2.bn2.num_batches_tracked\", \"decoder.p5.weight\", \"decoder.p5.bias\", \"decoder.p4.skip_conv.weight\", \"decoder.p4.skip_conv.bias\", \"decoder.p3.skip_conv.weight\", \"decoder.p3.skip_conv.bias\", \"decoder.p2.skip_conv.weight\", \"decoder.p2.skip_conv.bias\", \"decoder.seg_blocks.0.block.0.block.0.weight\", \"decoder.seg_blocks.0.block.0.block.1.weight\", \"decoder.seg_blocks.0.block.0.block.1.bias\", \"decoder.seg_blocks.0.block.1.block.0.weight\", \"decoder.seg_blocks.0.block.1.block.1.weight\", \"decoder.seg_blocks.0.block.1.block.1.bias\", \"decoder.seg_blocks.0.block.2.block.0.weight\", \"decoder.seg_blocks.0.block.2.block.1.weight\", \"decoder.seg_blocks.0.block.2.block.1.bias\", \"decoder.seg_blocks.1.block.0.block.0.weight\", \"decoder.seg_blocks.1.block.0.block.1.weight\", \"decoder.seg_blocks.1.block.0.block.1.bias\", \"decoder.seg_blocks.1.block.1.block.0.weight\", \"decoder.seg_blocks.1.block.1.block.1.weight\", \"decoder.seg_blocks.1.block.1.block.1.bias\", \"decoder.seg_blocks.2.block.0.block.0.weight\", \"decoder.seg_blocks.2.block.0.block.1.weight\", \"decoder.seg_blocks.2.block.0.block.1.bias\", \"decoder.seg_blocks.3.block.0.block.0.weight\", \"decoder.seg_blocks.3.block.0.block.1.weight\", \"decoder.seg_blocks.3.block.0.block.1.bias\", \"segmentation_head.0.weight\", \"segmentation_head.0.bias\". "
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "logs=[]\n",
    "print(\"Round 0\")\n",
    "strategy.train()\n",
    "logits, mask_gt = strategy.predict(data.get_test_data())\n",
    "logs.append(f\"Round 0 testing metrics: {data.cal_test_metrics(logits, mask_gt )}\")\n",
    "print(logs[0])\n",
    "\n",
    "for rd in range(1, params[\"rounds\"]):\n",
    "    print(f\"Round {rd}\")\n",
    "\n",
    "    # query\n",
    "    print(\"Querying\")\n",
    "    query_idxs = strategy.query(params[\"query_num\"])\n",
    "    print(query_idxs)\n",
    "\n",
    "    # update labels\n",
    "    if params[\"use_sam\"] and rd >= params[\"activate_sam_at_round\"]:\n",
    "        print(\"Updating with sam\")\n",
    "        strategy.update(query_idxs, start_sam=True, use_predictor=params[\"use_predictor\"], use_generator=params[\"use_generator\"])\n",
    "    else:\n",
    "        print(\"Updating without sam\")\n",
    "        strategy.update(query_idxs)\n",
    "    \n",
    "    print(\"Reset and train\")\n",
    "    # init_state = torch.load('init_state.pt')\n",
    "    # strategy.net.net.load_state_dict(init_state)\n",
    "    strategy.train()\n",
    "\n",
    "    # calculate accuracy\n",
    "    logits, maks_gt = strategy.predict(data.get_test_data())\n",
    "    logs.append(f\"Round {rd} testing metrics: {data.cal_test_metrics(logits, mask_gt)}\")\n",
    "    print(logs[rd])\n",
    "    \n",
    "params['logs'] = logs\n",
    "\n",
    "for dirname, _, filenames in os.walk(expirements_path):\n",
    "    filename = \"expirement_{}.json\".format(len(filenames))\n",
    "    file_path = os.path.join(dirname, filename)\n",
    "    with open(file_path, 'w') as f:\n",
    "        json.dump(params, f)\n",
    "        print(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0626ed57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed700bdc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc13bab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"segformer-b0-scene-parse-150\",\n",
    "    learning_rate=6e-5,\n",
    "    num_train_epochs=50,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    save_total_limit=3,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=20,\n",
    "    eval_steps=20,\n",
    "    logging_steps=1,\n",
    "    eval_accumulation_steps=5,\n",
    "    remove_unused_columns=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ee68a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=test_ds,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3bba28",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds[0]['pixel_values'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6fc908",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "al_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
